{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title: NPFL122, Lecture 10\n",
    "class: title, langtech, cc-by-nc-sa\n",
    "# TD3, Monte Carlo Tree Search\n",
    "\n",
    "## Milan Straka\n",
    "\n",
    "### December 17, 2018\n",
    "\n",
    "---\n",
    "section: Refresh\n",
    "# Deterministic Policy Gradient Theorem\n",
    "\n",
    "Combining continuous actions and Deep Q Networks is not straightforward.\n",
    "In order to do so, we need a different variant of the policy gradient theorem.\n",
    "\n",
    "Recall that in policy gradient theorem,\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "## Deterministic Policy Gradient Theorem\n",
    "Assume that the policy $π(s; →θ)$ is deterministic and computes\n",
    "an action $a∈ℝ$. Then under several assumptions about continuousness, the\n",
    "following holds:\n",
    "$$∇_→θ J(→θ) ∝ 𝔼_{s∼μ(s)} \\Big[∇_→θ π(s; →θ) ∇_a q_π(s, a)\\big|_{a=π(s;→θ)}\\Big].$$\n",
    "\n",
    "The theorem was first proven in the paper Deterministic Policy Gradient Algorithms\n",
    "by David Silver et al.\n",
    "\n",
    "---\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "Note that the formulation of deterministic policy gradient theorem allows an\n",
    "off-policy algorithm, because the loss functions no longer depends on actions\n",
    "(similarly to how expected Sarsa is also an off-policy algorithm).\n",
    "\n",
    "We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,\n",
    "training $q(s, a; →θ)$ using a deterministic variant of the Bellman equation:\n",
    "$$q(S_t, A_t; →θ) = 𝔼_{R_{t+1}, S_{t+1}} \\big[R_{t+1} + γ q(S_{t+1}, π(S_{t+1}; →θ))\\big]$$\n",
    "and $π(s; →θ)$ according to the deterministic policy gradient theorem.\n",
    "\n",
    "The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning\n",
    "by Timothy P. Lillicrap et al. (2015).\n",
    "\n",
    "The authors utilize a replay buffer, a target network (updated by exponential\n",
    "moving average with $τ=0.001$), batch normalization for CNNs, and perform\n",
    "exploration by adding a normal-distributed noise to predicted actions.\n",
    "Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the\n",
    "policy and critic network, respectively.\n",
    "\n",
    "---\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "![w=65%,h=center](images/../09/ddpg.png)\n",
    "\n",
    "---\n",
    "section: TD3\n",
    "# Twin Delayed Deep Deterministic Policy Gradient\n",
    "\n",
    "The paper Addressing Function Approximation Error in Actor-Critic Methods by\n",
    "Scott Fujimoto et al. from February 2018 proposes improvements to DDPG which\n",
    "\n",
    "- decrease maximization bias by training two critics and choosing minimum of\n",
    "  their predictions;\n",
    "\n",
    "- introduce several variance-lowering optimizations:\n",
    "  - delayed policy updates;\n",
    "  - target policy smoothing.\n",
    "\n",
    "---\n",
    "# TD3 – Maximization Bias\n",
    "\n",
    "Similarly to Q-learning, the DDPG algorithm suffers from maximization bias.\n",
    "In Q-learning, the maximization bias was caused by the explicit $\\max$ operator.\n",
    "For DDPG methods, it can be caused by the gradient descent itself. Let\n",
    "$→θ_\\textit{approx}$ be the parameters maximizing the $q_→θ$ and let\n",
    "$→θ_\\textit{true}$ be the hypothetical parameters which maximise true $q_π$,\n",
    "and let $π_\\textit{approx}$ and $π_\\textit{true}$ denote the corresponding\n",
    "policies.\n",
    "\n",
    "\n",
    "Because the gradient direction is a local maximizer, for sufficiently small\n",
    "$α<ε_1$ we have\n",
    "$$𝔼\\big[q_→θ(s, π_\\textit{approx})\\big] ≥ 𝔼\\big[q_→θ(s, π_\\textit{true})\\big].$$\n",
    "\n",
    "\n",
    "However, for real $q_π$ and for sufficiently small $α<ε_2$ it holds that\n",
    "$$𝔼\\big[q_π(s, π_\\textit{true})\\big] ≥ 𝔼\\big[q_π(s, π_\\textit{approx})\\big].$$\n",
    "\n",
    "\n",
    "Therefore, if $𝔼\\big[q_→θ(s, π_\\textit{true})\\big] ≥ 𝔼\\big[q_π(s, π_\\textit{true})\\big]$,\n",
    "for $α < \\min(ε_1, ε_2)$\n",
    "$$𝔼\\big[q_→θ(s, π_\\textit{approx})\\big] ≥ 𝔼\\big[q_π(s, π_\\textit{approx})\\big].$$\n",
    "\n",
    "---\n",
    "# TD3 – Maximization Bias\n",
    "\n",
    "![w=50%](images/td3_bias.png)![w=50%](images/td3_bias_dqac.png)\n",
    "\n",
    "\n",
    "Analogously to Double DQN we could compute the learning targets using\n",
    "the current policy and the target critic, i.e., $r + γ q_{→θ'}(s', π_→θ(s'))$\n",
    "(instead of using target policy and target critic as in DDPG), obtaining DDQN-AC algorithm.\n",
    "However, the authors found out that the policy changes too slowly and the target\n",
    "and current networks are too similar.\n",
    "\n",
    "\n",
    "Using the original Double Q-learning, two pairs of actors and critics could be\n",
    "used, with the learning targets computed by the opposite critic, i.e.,\n",
    "$r + γ q_{→θ'_2}(s', π_{→θ_1}(s))$ for updating $q_{→θ_1}$. The resulting DQ-AC\n",
    "algorithm is slightly better, but still suffering from oversetimation.\n",
    "\n",
    "---\n",
    "# TD3 – Algorithm\n",
    "\n",
    "The authors instead suggest to employ two critics and one actor. The actor is\n",
    "trained using one of the critics, and both critics are trained using the same\n",
    "target computed using the _minimum_ value of both critics as\n",
    "$$r + γ \\min_{i=1,2} q_{→θ'_i}(s', π_{→θ}(s')).$$\n",
    "\n",
    "\n",
    "Furthermore, the authors suggest two additional improvements for variance\n",
    "reduction.\n",
    "- For obtaining higher quality target values, the authors propose to train the\n",
    "  critics more often. Therefore, critics are updated each step, but the actor\n",
    "  and the target networks are updated only every $d$-th step ($d=2$ is used in\n",
    "  the paper).\n",
    "\n",
    "\n",
    "- To explictly model that similar actions should lead to similar results,\n",
    "  a small random noise is added to performed actions when computing the target\n",
    "  value:\n",
    "  $$r + γ \\min_{i=1,2} q_{→θ'_i}(s', π_{→θ}(s') + ε)\\textrm{for}\n",
    "    ε ∼ \\operatorname{clip}(𝓝(0, σ), -c, c).$$\n",
    "\n",
    "---\n",
    "# TD3 – Algorithm\n",
    "\n",
    "![w=43%,h=center](images/td3_algorithm.png)\n",
    "\n",
    "---\n",
    "# TD3 – Algorithm\n",
    "\n",
    "![w=80%,h=center](images/td3_hyperparameters.png)\n",
    "\n",
    "---\n",
    "# TD3 – Results\n",
    "\n",
    "![w=70%,h=center](images/td3_results_curves.png)\n",
    "![w=70%,h=center](images/td3_results.png)\n",
    "\n",
    "---\n",
    "# TD3 – Ablations\n",
    "\n",
    "![w=100%,h=center](images/td3_ablations.png)\n",
    "![w=100%,h=center](images/td3_ablations_dqac.png)\n",
    "\n",
    "---\n",
    "# TD3 – Ablations\n",
    "\n",
    "![w=65%,h=center](images/td3_ablations_results.png)\n",
    "\n",
    "---\n",
    "section: AlphaZero\n",
    "# AlphaZero\n",
    "\n",
    "On 7 December 2018, the AlphaZero paper came out in Science journal. It\n",
    "demonstrates learning chess, shogi and go, _tabula rasa_ – without any\n",
    "domain-specific human knowledge or data, only using self-play. The evaluation\n",
    "is performed against strongest programs available.\n",
    "\n",
    "![w=80%,h=center](images/a0_results.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Overview\n",
    "\n",
    "AlphaZero uses a neural network which using the current state $s$ predicts\n",
    "$(→p, v) = f(s; →θ)$, where:\n",
    "- $→p$ is a vector of move probabilities, and\n",
    "- $v$ is expected outcome of the game in range $[-1, 1]$.\n",
    "\n",
    "\n",
    "Instead of usual alpha-beta search used by classical game playing programs,\n",
    "AlphaZero uses Monte Carlo Tree Search (MCTS). By a sequence of simulated\n",
    "self-play games, the search can improve the estimate of $→p$ and $v$, and can be\n",
    "considered a powerful policy evaluation operator.\n",
    "\n",
    "\n",
    "The network is trained from self-play games. The game is played by repeatedly\n",
    "running MCTS from the state $s_t$ and choosing a move $a_t ∼ →π_t$, until\n",
    "a terminal position $s_T$ is encountered, which is scored according to game\n",
    "rules as $z∈\\{-1, 0, 1\\}$. Finally, the network parameters are trained to\n",
    "minimize the error between the predicted outcome $v$ and simulated outcome $z$,\n",
    "and maximize the similarity of the policy vector $→p_t$ and the search\n",
    "probabilities $→π_t$:\n",
    "$$L ≝ (z - v)^2 + →π^T \\log →p + c||→θ||^2.$$\n",
    "\n",
    "---\n",
    "section: A0-MCTS\n",
    "# AlphaZero – Monte Carlo Tree Search\n",
    "\n",
    "MCTS keeps a tree of currently explored states from a fixed root state.\n",
    "Each node corresponds to a game state. Each state-action pair $(s, a)$\n",
    "stores the following set of statistics:\n",
    "- visit count $N(s, a)$,\n",
    "- total action-value $W(s, a)$,\n",
    "- mean action value $Q(s, a) ≝ W(s, a) / N(s, a)$,\n",
    "- prior probability $P(s, a)$ of selecting action $a$ in state $s$.\n",
    "\n",
    "\n",
    "Each simulation starts in the root node and finishes in a leaf node $s_L$.\n",
    "In a state $s_t$, an action is selected using a variant of PUCT algorithm as\n",
    "$a_t = \\argmax\\nolimits_a (Q(s_t, a) + U(s_t, a))$, where\n",
    "$$U(s, a) ≝ C(s) P(s, a) \\frac{\\sqrt{N(s)}}{1 + N(s, a)}$$\n",
    "with $C(s) = \\log((1+N(s)+c_\\textrm{base}) / c_\\textrm{base}) + c_\\textrm{init}$\n",
    "being slightly time-increasing exploration rate. Additionally, exploration in\n",
    "$s_\\textrm{root}$ is supported by $P(s_\\textrm{root}, a) = (1-ε)p_a + ε\\operatorname{Dir}(α)$,\n",
    "with $ε=0.25$ and $α=0.3, 0.15, 0.03$ for chess, shogi and go, respectively.\n",
    "\n",
    "\n",
    "---\n",
    "# AlphaZero – Monte Carlo Tree Search\n",
    "\n",
    "When reaching a leaf node, it is evaluated by the network producing $(→p, v)$\n",
    "and all its children are initialized to $N=W=Q=0$, $P=→p$, and in the backward\n",
    "pass for all $t ≤ L$ the statistics are updates using $N(s_t, a_t) ← N(s_t, a_t) + 1$\n",
    "and $W(s_t, a_t) ← W(s_t, a_t) + v$.\n",
    "\n",
    "\n",
    "![w=100%,mh=70%,v=middle](images/ag0_mcts.png)\n",
    "\n",
    "\n",
    "Finally, the search probabilities in the root are defined as\n",
    "$→π_\\textrm{root} ∝ N(s_\\textrm{root}, ⋅)$.\n",
    "\n",
    "---\n",
    "section: A0-Network\n",
    "# AlphaZero – Network Architecture\n",
    "\n",
    "The network processes game-specific input, which consists of a history of\n",
    "8 board positions encoded by several $N × N$ planes, and some number of\n",
    "constant-valued inputs.\n",
    "\n",
    "\n",
    "Output is considered to be a categorical distribution of possible moves. For\n",
    "chess and shogi, for each piece we consider all possible moves (56 queen\n",
    "moves, 8 knight moves and 9 underpromotions for chess).\n",
    "\n",
    "\n",
    "The input is processed by:\n",
    "- initial convolution block with CNN with 256 $3×3$ kernels with stride 1, batch\n",
    "  normalization and ReLU activation,\n",
    "\n",
    "- 19 residual blocks, each consisting of two CNN with 256 $3×3$ kernels with stride 1,\n",
    "  batch normalization and ReLU activation, and a residual connection around\n",
    "  them,\n",
    "\n",
    "- _policy head_, which applies another CNN with batch normalization, followed by\n",
    "  a convolution with 73/139 filters for chess/shogi, or a linear layer of size\n",
    "  362 for go,\n",
    "\n",
    "- _value head_, which applies another CNN with 1 $1×1$ kernel with stride 1,\n",
    "  followed by a ReLU layer of size 256 and final $\\tanh$ layer of size 1.\n",
    "\n",
    "---\n",
    "# AlphaZero – Network Inputs\n",
    "\n",
    "![w=80%,h=center,v=middle](images/a0_input.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Network Outputs\n",
    "\n",
    "![w=80%,h=center,v=middle](images/a0_output.png)\n",
    "\n",
    "---\n",
    "section: A0-Training\n",
    "# AlphaZero – Training\n",
    "\n",
    "Training is performed by running self-play games of the network with itself.\n",
    "Each MCTS uses 800 simulations. A replay buffer of one million most recent games\n",
    "is kept.\n",
    "\n",
    "\n",
    "During training, 5000 first-generation TPUs are used to generate self-play games.\n",
    "Simultaneously, network is trained using SGD with momentum of 0.9 on batches\n",
    "of size 4096, utilizing 16 second-generation TPUs. Training takes approximately\n",
    "9 hours for chess, 12 hours for shogi and 13 days for go.\n",
    "\n",
    "---\n",
    "# AlphaZero – Training\n",
    "\n",
    "![w=100%](images/a0_results_learning.png)\n",
    "\n",
    "![w=85%,h=center](images/a0_training_stats.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Training\n",
    "\n",
    "According to the authors, training is highly repeatable.\n",
    "\n",
    "![w=50%,h=center,mh=90%,v=middle](images/a0_repeatability.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Symmetries\n",
    "\n",
    "In the original AlphaGo Zero, symmetries were explicitly utilized, by\n",
    "- randomly sampling a symmetry during training,\n",
    "- randomly sampling a symmetry during evaluation.\n",
    "\n",
    "However, AlphaZero does not utilize symmetries in any way (because chess and\n",
    "shogi do not have them).\n",
    "\n",
    "![w=100%,h=center](images/a0_symmetries.png)\n",
    "\n",
    "---\n",
    "section: A0-Evaluation\n",
    "# AlphaZero – Inference\n",
    "\n",
    "During inference, AlphaZero utilizes much less evaluations than classical game\n",
    "playing programs.\n",
    "\n",
    "![w=80%,h=center,mh=90%,v=middle](images/a0_inference.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Ablations\n",
    "\n",
    "![w=65%,h=center](images/a0_match_chess.png)\n",
    "![w=65%,h=center](images/a0_match_shogi.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Ablations\n",
    "\n",
    "![w=80%,h=center](images/a0_results_ablations.png)\n",
    "\n",
    "---\n",
    "# AlphaZero – Ablations\n",
    "\n",
    "![w=100%](images/ag0_architectures_ablation.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
