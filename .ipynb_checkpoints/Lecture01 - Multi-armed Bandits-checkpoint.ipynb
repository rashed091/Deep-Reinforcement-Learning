{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Multi-armed Bandits\n",
    "\n",
    "<img src=\"images/one-armed-bandit.jpg\" width=\"60%\">\n",
    "\n",
    "\n",
    "<img src=\"images/k.png\" width=\"60%\">\n",
    "\n",
    "\n",
    "We start by selecting action $A_1$, which is the index of the arm to use, and we\n",
    "get a reward of $R_1$. We then repeat the process by selecting actions $A_2$, $A_3$, ‚Ä¶\n",
    "\n",
    "Let $q_*(a)$ be the real _value_ of an action $a$:\n",
    "$$q_*(a) = ùîº[R_t | A_t = a].$$\n",
    "\n",
    "\n",
    "Denoting $Q_t(a)$ our estimated value of action $a$ at time $t$ (before taking\n",
    "trial $t$), we would like $Q_t(a)$ to converge to $q_*(a)$. A natural way to\n",
    "estimate $Q_t(a)$ is\n",
    "$$Q_t(a) ‚âù \\frac{\\textrm{sum of rewards when action }a\\textrm{ is taken}}{\\textrm{number of times action }a\\textrm{ was taken}}.$$\n",
    "\n",
    "\n",
    "Following the definition of $Q_t(a)$, we could choose a _greedy action_ $A_t$ as\n",
    "$$A_t(a) ‚âù {argmax}_a Q_t(a).$$\n",
    "\n",
    "\n",
    "# $Œµ$-greedy Method\n",
    "\n",
    "## Exploitation versus Exploration\n",
    "\n",
    "Choosing a greedy action is _exploitation_ of current estimates. We however also\n",
    "need to _explore_ the space of actions to improve our estimates.\n",
    "\n",
    "\n",
    "An _$Œµ$-greedy_ method follows the greedy action with probability $1-Œµ$, and\n",
    "chooses a uniformly random action with probability $Œµ$.\n",
    "\n",
    "# $Œµ$-greedy Method\n",
    "<img src=\"images/e_greedy.png\" width=\"60%\">\n",
    "\n",
    "# $Œµ$-greedy Method\n",
    "\n",
    "## Incremental Implementation\n",
    "\n",
    "Let $Q_{n+1}$ be an estimate using $n$ rewards $R_1, \\ldots, R_n$.\n",
    "\n",
    "$$\\begin{aligned}\n",
    "Q_{n+1} &= \\frac{1}{n} ‚àë_{i=1}^n R_i \\\\\n",
    "    &= \\frac{1}{n} (R_n + \\frac{n-1}{n-1} ‚àë_{i=1}^{n-1} R_i) \\\\\n",
    "    &= \\frac{1}{n} (R_n + (n-1) Q_n) \\\\\n",
    "    &= \\frac{1}{n} (R_n + n Q_n - Q_n) \\\\\n",
    "    &= Q_n + \\frac{1}{n}\\Big(R_n - Q_n\\Big)\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "# $Œµ$-greedy Method Algorithm\n",
    "\n",
    "\n",
    "<img src=\"images/bandits_algorithm.png\" width=\"60%\">\n",
    "section: Non-stationary Problems\n",
    "# Fixed Learning Rate\n",
    "\n",
    "Analogously to the solution obtained for a stationary problem, we consider\n",
    "$$Q_{n+1} = Q_n + Œ±(R_n - Q_n).$$\n",
    "\n",
    "\n",
    "Converges to the true action values if\n",
    "$$‚àë_{n=1}^‚àû Œ±_n = ‚àû \\textrm{~~~~and~~~~}‚àë_{n=1}^‚àû Œ±_n^2 < ‚àû.$$\n",
    "\n",
    "\n",
    "Biased method, because\n",
    "$$Q_{n+1} = (1 - Œ±)^n Q_1 + ‚àë_{i=1}^n Œ±(1-Œ±)^{n-i} R_i.$$\n",
    "\n",
    "\n",
    "The bias can be utilized to support exploration at the start of the episode by\n",
    "setting the initial values to more than the expected value of the optimal\n",
    "solution.\n",
    "\n",
    "\n",
    "# Optimistic Initial Values and Fixed Learning Rate\n",
    "\n",
    "\n",
    "<img src=\"images/optimistic_values.png\" width=\"60%\">\n",
    "# Upper Confidence Bound\n",
    "\n",
    "Using same epsilon for all actions in $Œµ$-greedy method seems inefficient. One\n",
    "possible improvement is to select action according to upper confidence bound\n",
    "(instead of choosing a random action with probability $Œµ$):\n",
    "$$A_t ‚âù {argmax}_a \\left[Q_t(a) + c\\sqrt{\\frac{\\ln t}{N_t(a)}}\\right].$$\n",
    "\n",
    "The updates are then performed as before (e.g., using averaging, or fixed\n",
    "learning rate $Œ±$).\n",
    "\n",
    "\n",
    "# Motivation Behind Upper Confidence Bound\n",
    "\n",
    "Actions with little average reward are probably selected too often.\n",
    "\n",
    "\n",
    "Instead of simple $Œµ$-greedy approach, we might try selecting an action as\n",
    "little as possible, but still enough to converge.\n",
    "\n",
    "\n",
    "Assuming random variables $X_i$ bounded by $[0, 1]$ and $XÃÑ = ‚àë_{i=1}^N\n",
    "X_i$, (Chernoff-)Hoeffding's inequality states that\n",
    "$$P(XÃÑ - ùîº[XÃÑ] ‚â• Œ¥) ‚â§ e^{-2nŒ¥^2}.$$\n",
    "\n",
    "\n",
    "Our goal is to choose $Œ¥$ such that for every action,\n",
    "$$P(Q_t(a) - q_*(a) ‚â• Œ¥) ‚â§ \\left(\\frac{1}{t}\\right)^Œ±.$$\n",
    "\n",
    "\n",
    "We can achieve the required inequality (with $Œ±=2$) by setting\n",
    "$$Œ¥ ‚â• \\sqrt{(\\ln t)/N_t(a)}.$$\n",
    "\n",
    "\n",
    "# Asymptotical Optimality of UCB\n",
    "\n",
    "We define _regret_ as a difference of maximum of what we could get\n",
    "(i.e., repeatedly using action with maximum expectation) and\n",
    "what a strategy yields, i.e.,\n",
    "$$\\textit{regret}_N ‚âù N \\max_a q_*(a) - ‚àë_{i=1}^N ùîº[R_i].$$\n",
    "\n",
    "\n",
    "It can be shown that regret of UCB is asymptotically optimal,\n",
    "see Lai and Robbins (1985), Asymptotically Efficient Adaptive Allocation Rules.\n",
    "\n",
    "\n",
    "# Upper Confidence Bound Results\n",
    "\n",
    "<img src=\"images/ucb.png\" width=\"60%\">\n",
    "\n",
    "# Gradient Bandit Algorithms\n",
    "\n",
    "Let $H_t(a)$ be a numerical _preference_ for an action $a$ at time $t$.\n",
    "\n",
    "We could choose actions according to softmax distribution:\n",
    "$$œÄ(A_t = a) ‚âù {softmax}(a) = \\frac{e^{H_t(a)}}{‚àë_b e^{H_t(b)}}.$$\n",
    "\n",
    "\n",
    "Usually, all $H_1(a)$ are set to zero, which corresponds to random uniform\n",
    "initial policy.\n",
    "\n",
    "\n",
    "Using SGD and MLE loss, we can derive the following algorithm:\n",
    "$$H_{t+1}(a) ‚Üê H_t(a) + Œ±R_t([a = A_t] - œÄ(a)).$$\n",
    "\n",
    "# Gradient Bandit Algorithms\n",
    "\n",
    "<img src=\"images/gradient_bandits.png\" width=\"60%\">\n",
    "\n",
    "# Method Comparison\n",
    "\n",
    "<img src=\"images/bandits_comparison.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiArmedBandits():\n",
    "    def __init__(self, bandits, episode_length):\n",
    "        self._bandits = []\n",
    "        for _ in range(bandits):\n",
    "            self._bandits.append(np.random.normal(0., 1.))\n",
    "        self._done = True\n",
    "        self._episode_length = episode_length\n",
    "        #print(\"Initialized {}-armed bandit, maximum average reward is {}\".format(bandits, np.max(self._bandits)))\n",
    "\n",
    "    def reset(self):\n",
    "        self._done = False\n",
    "        self._trials = 0\n",
    "        return None\n",
    "\n",
    "    def step(self, action):\n",
    "        if self._done:\n",
    "            raise ValueError(\"Cannot step in MultiArmedBandits when there is no running episode\")\n",
    "        self._trials += 1\n",
    "        self._done = self._trials == self._episode_length\n",
    "        reward = np.random.normal(self._bandits[action], 1.)\n",
    "        return None, reward, self._done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Fix random seed\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parse arguments\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--bandits\", default=10, type=int, help=\"Number of bandits.\")\n",
    "    parser.add_argument(\"--episodes\", default=1000, type=int, help=\"Training episodes.\")\n",
    "    parser.add_argument(\"--episode_length\", default=1000, type=int, help=\"Number of trials per episode.\")\n",
    "\n",
    "    parser.add_argument(\"--mode\", default=\"greedy\", type=str, help=\"Mode to use -- greedy, ucb and gradient.\")\n",
    "    parser.add_argument(\"--alpha\", default=0, type=float, help=\"Learning rate to use (if applicable).\")\n",
    "    parser.add_argument(\"--c\", default=1., type=float, help=\"Confidence level in ucb.\")\n",
    "    parser.add_argument(\"--epsilon\", default=0.1, type=float, help=\"Exploration factor (if applicable).\")\n",
    "    parser.add_argument(\"--initial\", default=0, type=float, help=\"Initial value function levels.\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = MultiArmedBandits(args.bandits, args.episode_length)\n",
    "\n",
    "    average_rewards = []\n",
    "    for episode in range(args.episodes):\n",
    "        env.reset()\n",
    "\n",
    "        # TODO: Initialize required values (depending on mode).\n",
    "\n",
    "        average_rewards.append(0)\n",
    "        done = False\n",
    "        while not done:\n",
    "            # TODO: Action selection according to mode\n",
    "            if args.mode == \"greedy\":\n",
    "                action =\n",
    "            elif args.mode == \"ucb\":\n",
    "                action =\n",
    "            elif args.mode == \"gradient\":\n",
    "                action =\n",
    "\n",
    "            _, reward, done, _ = env.step(action)\n",
    "            average_rewards[-1] += reward / args.episode_length\n",
    "\n",
    "            # TODO: Update parameters\n",
    "\n",
    "    # Print out final score as mean and variance of all obtained rewards.\n",
    "    print(\"Final score: {}, variance: {}\".format(np.mean(average_rewards), np.var(average_rewards)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
