{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "section: Policy Gradient Methods\n",
    "# Policy Gradient Methods\n",
    "\n",
    "Instead of predicting expected returns, we could train the method to directly\n",
    "predict the policy\n",
    "$$Ï€(a | s; â†’Î¸).$$\n",
    "\n",
    "\n",
    "Obtaining the full distribution over all actions would also allow us to sample\n",
    "the actions according to the distribution $Ï€$ instead of just $Îµ$-greedy\n",
    "sampling.\n",
    "\n",
    "\n",
    "However, to train the network, we maximize the expected return $v_Ï€(s)$ and to\n",
    "that account we need to compute its _gradient_ $âˆ‡_â†’Î¸ v_Ï€(s)$.\n",
    "\n",
    "---\n",
    "# Policy Gradient Methods\n",
    "\n",
    "In addition to discarding $Îµ$-greedy action selection, policy gradient methods\n",
    "allow producing policies which are by nature stochastic, as in card games with\n",
    "imperfect information, while the action-value methods have no natural way of\n",
    "finding stochastic policies (distributional RL might be of some use though).\n",
    "\n",
    "\n",
    "![w=75%,h=center](images/stochastic_policy_example.png)\n",
    "\n",
    "---\n",
    "# Policy Gradient Theorem\n",
    "\n",
    "Let $Ï€(a | s; â†’Î¸)$ be a parametrized policy. We denote the initial state\n",
    "distribution as $h(s)$ and the on-policy distribution under $Ï€$ as $Î¼(s)$.\n",
    "Let also $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$.\n",
    "\n",
    "\n",
    "Then\n",
    "$$âˆ‡_â†’Î¸ v_Ï€(s) âˆ âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸)$$\n",
    "and\n",
    "$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$\n",
    "\n",
    "\n",
    "where $P(s â†’ â€¦ â†’ s'|Ï€)$ is probability of transitioning from state $s$ to $s'$\n",
    "using 0, 1, â€¦ steps.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "# Proof of Policy Gradient Theorem\n",
    "\n",
    "$\\displaystyle âˆ‡v_Ï€(s) = âˆ‡ \\Big[ âˆ‘\\nolimits_a Ï€(a|s; â†’Î¸) q_Ï€(s, a) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{âˆ‡v_Ï€(s)} = âˆ‘\\nolimits_a \\Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ q_Ï€(s, a) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{âˆ‡v_Ï€(s)} = âˆ‘\\nolimits_a \\Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) âˆ‡ \\big(âˆ‘\\nolimits_{s'} p(s'|s, a)(r + v_Ï€(s'))\\big) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{âˆ‡v_Ï€(s)} = âˆ‘\\nolimits_a \\Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \\big(âˆ‘\\nolimits_{s'} p(s'|s, a) âˆ‡ v_Ï€(s')\\big) \\Big]$\n",
    "\n",
    "\n",
    "_We now expand $v_Ï€(s')$._\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{âˆ‡v_Ï€(s)} = âˆ‘\\nolimits_a \\Big[ âˆ‡ Ï€(a|s; â†’Î¸) q_Ï€(s, a) + Ï€(a|s; â†’Î¸) \\Big(âˆ‘\\nolimits_{s'} p(s'|s, a)\\Big(\\\\\n",
    "                \\qquad\\qquad\\qquad âˆ‘\\nolimits_{a'} \\Big[ âˆ‡ Ï€(a'|s'; â†’Î¸) q_Ï€(s', a') + Ï€(a'|s'; â†’Î¸) \\Big(âˆ‘\\nolimits_{s''} p(s''|s', a') âˆ‡ v_Ï€(s'')\\Big) \\big) \\Big]$\n",
    "\n",
    "\n",
    "_Continuing to expand all $v_Ï€(s'')$, we obtain the following:_\n",
    "\n",
    "$\\displaystyle âˆ‡v_Ï€(s) = âˆ‘_{s'âˆˆğ“¢} P(s â†’ â€¦ â†’ s'|Ï€) âˆ‘_{a âˆˆ ğ“} q_Ï€(s', a) âˆ‡_â†’Î¸ Ï€(a | s'; â†’Î¸).$\n",
    "\n",
    "---\n",
    "# Proof of Policy Gradient Theorem\n",
    "\n",
    "Recall that the initial state distribution is $h(s)$ and the on-policy\n",
    "distribution under $Ï€$ is $Î¼(s)$. If we let $Î·(s)$ denote the number\n",
    "of time steps spent, on average, in state $s$ in a single episode,\n",
    "we have\n",
    "$$Î·(s) = h(s) + âˆ‘_{s'}Î·(s') âˆ‘_a Ï€(a|s') p(s|s',a).$$\n",
    "\n",
    "\n",
    "The on-policy distribution is then the normalization of $Î·(s)$:\n",
    "$$Î¼(s) â‰ \\frac{Î·(s)}{âˆ‘_{s'} Î·(s')}.$$\n",
    "\n",
    "\n",
    "The last part of the policy gradient theorem follows from the fact that $Î¼(s)$ is\n",
    "$$Î¼(s) = ğ”¼_{s_0 âˆ¼ h(s)} P(s_0 â†’ â€¦ â†’ s | Ï€).$$\n",
    "\n",
    "---\n",
    "section: REINFORCE\n",
    "# REINFORCE Algorithm\n",
    "\n",
    "The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient\n",
    "theorem, maximizing $J(â†’Î¸) â‰ ğ”¼_{h, Ï€} v_Ï€(s)$. The loss is defined as\n",
    "$$\\begin{aligned}\n",
    "  -âˆ‡_â†’Î¸ J(â†’Î¸) &âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) \\\\\n",
    "              &= ğ”¼_{s âˆ¼ Î¼} âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "However, the sum over all actions is problematic. Instead, we rewrite it to an\n",
    "expectation which we can estimate by sampling:\n",
    "$$-âˆ‡_â†’Î¸ J(â†’Î¸) âˆ ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \\ln Ï€(a | s; â†’Î¸),$$\n",
    "where we used the fact that\n",
    "$$âˆ‡_â†’Î¸ \\ln Ï€(a | s; â†’Î¸) = \\frac{1}{Ï€(a | s; â†’Î¸)} âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$\n",
    "\n",
    "To compute the gradient\n",
    "$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} q_Ï€(s, a) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸),$$\n",
    "\n",
    "---\n",
    "# REINFORCE Algorithm\n",
    "\n",
    "REINFORCE therefore minimizes the loss\n",
    "$$-ğ”¼_{s âˆ¼ Î¼} ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a) âˆ‡_â†’Î¸ \\ln Ï€(a | s; â†’Î¸),$$\n",
    "estimating the $q_Ï€(s, a)$ by a single sample.\n",
    "\n",
    "Note that the loss is just a weighted variant of negative log likelihood (NLL),\n",
    "where the sampled actions play a role of gold labels and are weighted according\n",
    "to their return.\n",
    "\n",
    "![w=75%,h=center](images/reinforce.png)\n",
    "\n",
    "---\n",
    "section: Baseline\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "The returns can be arbitrary â€“ better-than-average and worse-than-average\n",
    "returns cannot be recognized from the absolute value of the return.\n",
    "\n",
    "\n",
    "Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$\n",
    "to\n",
    "$$âˆ‡_â†’Î¸ J(â†’Î¸) âˆ âˆ‘_{sâˆˆğ“¢} Î¼(s) âˆ‘_{a âˆˆ ğ“} \\big(q_Ï€(s, a) - b(s)\\big) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸).$$\n",
    "\n",
    "\n",
    "The baseline $b(s)$ can be a function or even a random variable, as long as it\n",
    "does not depend on $a$, because\n",
    "$$âˆ‘_a b(s) âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‘_a âˆ‡_â†’Î¸ Ï€(a | s; â†’Î¸) = b(s) âˆ‡1 = 0.$$\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "A good choice for $b(s)$ is $v_Ï€(s)$, which can be shown to minimize variance of\n",
    "the estimator. Such baseline reminds centering of returns, given that\n",
    "$$v_Ï€(s) = ğ”¼_{a âˆ¼ Ï€} q_Ï€(s, a).$$\n",
    "\n",
    "\n",
    "Then, better-than-average returns are positive and worse-than-average returns\n",
    "are negative.\n",
    "\n",
    "\n",
    "The resulting $q_Ï€(s, a) - v_Ï€(s)$ function is also called an _advantage function_\n",
    "$$a_Ï€(s, a) â‰ q_Ï€(s, a) - v_Ï€(s).$$\n",
    "\n",
    "\n",
    "Of course, the $v_Ï€(s)$ baseline can be only approximated. If neural networks\n",
    "are used to estimate $Ï€(a|s; â†’Î¸)$, then some part of the network is usually\n",
    "shared between the policy and value function estimation, which is trained using\n",
    "mean square error of the predicted and observed return.\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "![w=100%](images/reinforce_with_baseline.png)\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "![w=100%](images/reinforce_with_baseline_comparison.png)\n",
    "\n",
    "---\n",
    "section: Actor-Critic\n",
    "# Actor-Critic\n",
    "\n",
    "It is possible to combine the policy gradient methods and temporal difference\n",
    "methods, creating a family of algorithms usually called _actor-critic_ methods.\n",
    "\n",
    "\n",
    "The idea is straightforward â€“ instead of estimating the episode return using the\n",
    "whole episode rewards, we can use $n$-step temporal difference estimation.\n",
    "\n",
    "---\n",
    "# Actor-Critic\n",
    "\n",
    "![w=85%,h=center](images/actor_critic.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
