{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "section: Policy Gradient Methods\n",
    "# Policy Gradient Methods\n",
    "\n",
    "Instead of predicting expected returns, we could train the method to directly\n",
    "predict the policy\n",
    "$$π(a | s; →θ).$$\n",
    "\n",
    "\n",
    "Obtaining the full distribution over all actions would also allow us to sample\n",
    "the actions according to the distribution $π$ instead of just $ε$-greedy\n",
    "sampling.\n",
    "\n",
    "\n",
    "However, to train the network, we maximize the expected return $v_π(s)$ and to\n",
    "that account we need to compute its _gradient_ $∇_→θ v_π(s)$.\n",
    "\n",
    "---\n",
    "# Policy Gradient Methods\n",
    "\n",
    "In addition to discarding $ε$-greedy action selection, policy gradient methods\n",
    "allow producing policies which are by nature stochastic, as in card games with\n",
    "imperfect information, while the action-value methods have no natural way of\n",
    "finding stochastic policies (distributional RL might be of some use though).\n",
    "\n",
    "\n",
    "![w=75%,h=center](images/stochastic_policy_example.png)\n",
    "\n",
    "---\n",
    "# Policy Gradient Theorem\n",
    "\n",
    "Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state\n",
    "distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.\n",
    "Let also $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.\n",
    "\n",
    "\n",
    "Then\n",
    "$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$\n",
    "and\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$\n",
    "\n",
    "\n",
    "where $P(s → … → s'|π)$ is probability of transitioning from state $s$ to $s'$\n",
    "using 0, 1, … steps.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "# Proof of Policy Gradient Theorem\n",
    "\n",
    "$\\displaystyle ∇v_π(s) = ∇ \\Big[ ∑\\nolimits_a π(a|s; →θ) q_π(s, a) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇v_π(s)} = ∑\\nolimits_a \\Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇v_π(s)} = ∑\\nolimits_a \\Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \\big(∑\\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\\big) \\Big]$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇v_π(s)} = ∑\\nolimits_a \\Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \\big(∑\\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\\big) \\Big]$\n",
    "\n",
    "\n",
    "_We now expand $v_π(s')$._\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇v_π(s)} = ∑\\nolimits_a \\Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \\Big(∑\\nolimits_{s'} p(s'|s, a)\\Big(\\\\\n",
    "                \\qquad\\qquad\\qquad ∑\\nolimits_{a'} \\Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \\Big(∑\\nolimits_{s''} p(s''|s', a') ∇ v_π(s'')\\Big) \\big) \\Big]$\n",
    "\n",
    "\n",
    "_Continuing to expand all $v_π(s'')$, we obtain the following:_\n",
    "\n",
    "$\\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} P(s → … → s'|π) ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$\n",
    "\n",
    "---\n",
    "# Proof of Policy Gradient Theorem\n",
    "\n",
    "Recall that the initial state distribution is $h(s)$ and the on-policy\n",
    "distribution under $π$ is $μ(s)$. If we let $η(s)$ denote the number\n",
    "of time steps spent, on average, in state $s$ in a single episode,\n",
    "we have\n",
    "$$η(s) = h(s) + ∑_{s'}η(s') ∑_a π(a|s') p(s|s',a).$$\n",
    "\n",
    "\n",
    "The on-policy distribution is then the normalization of $η(s)$:\n",
    "$$μ(s) ≝ \\frac{η(s)}{∑_{s'} η(s')}.$$\n",
    "\n",
    "\n",
    "The last part of the policy gradient theorem follows from the fact that $μ(s)$ is\n",
    "$$μ(s) = 𝔼_{s_0 ∼ h(s)} P(s_0 → … → s | π).$$\n",
    "\n",
    "---\n",
    "section: REINFORCE\n",
    "# REINFORCE Algorithm\n",
    "\n",
    "The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient\n",
    "theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. The loss is defined as\n",
    "$$\\begin{aligned}\n",
    "  -∇_→θ J(→θ) &∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ) \\\\\n",
    "              &= 𝔼_{s ∼ μ} ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "However, the sum over all actions is problematic. Instead, we rewrite it to an\n",
    "expectation which we can estimate by sampling:\n",
    "$$-∇_→θ J(→θ) ∝ 𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \\ln π(a | s; →θ),$$\n",
    "where we used the fact that\n",
    "$$∇_→θ \\ln π(a | s; →θ) = \\frac{1}{π(a | s; →θ)} ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "To compute the gradient\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$\n",
    "\n",
    "---\n",
    "# REINFORCE Algorithm\n",
    "\n",
    "REINFORCE therefore minimizes the loss\n",
    "$$-𝔼_{s ∼ μ} 𝔼_{a ∼ π} q_π(s, a) ∇_→θ \\ln π(a | s; →θ),$$\n",
    "estimating the $q_π(s, a)$ by a single sample.\n",
    "\n",
    "Note that the loss is just a weighted variant of negative log likelihood (NLL),\n",
    "where the sampled actions play a role of gold labels and are weighted according\n",
    "to their return.\n",
    "\n",
    "![w=75%,h=center](images/reinforce.png)\n",
    "\n",
    "---\n",
    "section: Baseline\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "The returns can be arbitrary – better-than-average and worse-than-average\n",
    "returns cannot be recognized from the absolute value of the return.\n",
    "\n",
    "\n",
    "Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$\n",
    "to\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \\big(q_π(s, a) - b(s)\\big) ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "\n",
    "The baseline $b(s)$ can be a function or even a random variable, as long as it\n",
    "does not depend on $a$, because\n",
    "$$∑_a b(s) ∇_→θ π(a | s; →θ) = b(s) ∑_a ∇_→θ π(a | s; →θ) = b(s) ∇1 = 0.$$\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of\n",
    "the estimator. Such baseline reminds centering of returns, given that\n",
    "$$v_π(s) = 𝔼_{a ∼ π} q_π(s, a).$$\n",
    "\n",
    "\n",
    "Then, better-than-average returns are positive and worse-than-average returns\n",
    "are negative.\n",
    "\n",
    "\n",
    "The resulting $q_π(s, a) - v_π(s)$ function is also called an _advantage function_\n",
    "$$a_π(s, a) ≝ q_π(s, a) - v_π(s).$$\n",
    "\n",
    "\n",
    "Of course, the $v_π(s)$ baseline can be only approximated. If neural networks\n",
    "are used to estimate $π(a|s; →θ)$, then some part of the network is usually\n",
    "shared between the policy and value function estimation, which is trained using\n",
    "mean square error of the predicted and observed return.\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "![w=100%](images/reinforce_with_baseline.png)\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "![w=100%](images/reinforce_with_baseline_comparison.png)\n",
    "\n",
    "---\n",
    "section: Actor-Critic\n",
    "# Actor-Critic\n",
    "\n",
    "It is possible to combine the policy gradient methods and temporal difference\n",
    "methods, creating a family of algorithms usually called _actor-critic_ methods.\n",
    "\n",
    "\n",
    "The idea is straightforward – instead of estimating the episode return using the\n",
    "whole episode rewards, we can use $n$-step temporal difference estimation.\n",
    "\n",
    "---\n",
    "# Actor-Critic\n",
    "\n",
    "![w=85%,h=center](images/actor_critic.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
