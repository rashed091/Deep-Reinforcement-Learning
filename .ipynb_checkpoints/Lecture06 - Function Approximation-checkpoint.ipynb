{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "section: Refresh\n",
    "# Function Approximation\n",
    "\n",
    "We will approximate value function $v$ and/or state-value function $q$, choosing\n",
    "from a family of functions parametrized by a weight vector $→w∈ℝ^d$.\n",
    "\n",
    "We denote the approximations as\n",
    "$$\\begin{gathered}\n",
    "  v̂(s, →w),\\\\\n",
    "  q̂(s, a, →w).\n",
    "\\end{gathered}$$\n",
    "\n",
    "\n",
    "We utilize the _Mean Squared Value Error_ objective, denoted $\\overline{VE}$:\n",
    "$$\\overline{VE}(→w) ≝ ∑_{s∈𝓢} μ(s) \\left[v_π(s) - v̂(s, →w)\\right]^2,$$\n",
    "where the state distribution $μ(s)$ is usually on-policy distribution.\n",
    "\n",
    "---\n",
    "# Gradient and Semi-Gradient Methods\n",
    "\n",
    "The functional approximation (i.e., the weight vector $→w$) is usually optimized\n",
    "using gradient methods, for example as\n",
    "$$\\begin{aligned}\n",
    "  →w_{t+1} &← →w_t - \\frac{1}{2} α ∇ \\left[v_π(S_t) - v̂(S_t, →w_t)\\right]^2\\\\\n",
    "           &← →w_t + α\\left[v_π(S_t) - v̂(S_t, →w_t)\\right] ∇ v̂(S_t, →w_t).\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "As usual, the $v_π(S_t)$ is estimated by a suitable sample. For example in Monte\n",
    "Carlo methods, we use episodic return $G_t$, and in temporal difference methods,\n",
    "we employ bootstrapping and use $R_{t+1} + γv̂(S_{t+1}, →w).$\n",
    "\n",
    "---\n",
    "section: DQN\n",
    "# Deep Q Network\n",
    "\n",
    "Off-policy Q-learning algorithm with a convolutional neural network function\n",
    "approximation of action-value function.\n",
    "\n",
    "Training can be extremely brittle (and can even diverge as shown earlier).\n",
    "\n",
    "![w=65%,h=center](images/dqn_architecture.png)\n",
    "\n",
    "---\n",
    "# Deep Q Networks\n",
    "\n",
    "- Preprocessing: $210×160$ 128-color images are converted to grayscale and\n",
    "  then resized to $84×84$.\n",
    "\n",
    "- Frame skipping technique is used, i.e., only every $4^\\textrm{th}$ frame\n",
    "  (out of 60 per second) is considered, and the selected action is repeated on\n",
    "  the other frames.\n",
    "\n",
    "- Input to the network are last $4$ frames (considering only the frames kept by\n",
    "  frame skipping), i.e., an image with $4$ channels.\n",
    "\n",
    "- The network is fairly standard, performing\n",
    "  - 32 filters of size $8×8$ with stride 4 and ReLU,\n",
    "  - 64 filters of size $4×4$ with stride 2 and ReLU,\n",
    "  - 64 filters of size $3×3$ with stride 1 and ReLU,\n",
    "  - fully connected layer with 512 units and ReLU,\n",
    "  - output layer with 18 output units (one for each action)\n",
    "\n",
    "---\n",
    "# Deep Q Networks\n",
    "\n",
    "- Network is trained with RMSProp to minimize the following loss:\n",
    "  $$𝓛 ≝ 𝔼_{(s, a, r, s')∼\\mathit{data}}\\left[(r + γ \\max_{a'} Q(s', a'; θ̄) - Q(s, a; θ))^2\\right].$$\n",
    "\n",
    "- An $ε$-greedy behavior policy is utilized.\n",
    "\n",
    "\n",
    "Important improvements:\n",
    "\n",
    "- experience replay: the generated episodes are stored in a buffer as $(s, a, r,\n",
    "  s')$ quadruples, and for training a transition is sampled uniformly;\n",
    "\n",
    "- separate target network $θ̄$: to prevent instabilities, a separate target\n",
    "  network is used to estimate state-value function. The weights are not trained,\n",
    "  but copied from the trained network once in a while;\n",
    "\n",
    "- reward clipping of $(r + γ \\max_{a'} Q(s', a'; θ̄) - Q(s, a; θ))$ to $[-1, 1]$.\n",
    "\n",
    "---\n",
    "class: tablefull\n",
    "# Deep Q Networks Hyperparameters\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| minibatch size | 32 |\n",
    "| replay buffer size | 1M |\n",
    "| target network update frequency | 10k |\n",
    "| discount factor | 0.99 |\n",
    "| training frames | 50M |\n",
    "| RMSProp learning rate and momentum | 0.00025, 0.95 |\n",
    "| initial $ε$, final $ε$ and frame of final $ε$ | 1.0, 0.1, 1M |\n",
    "| replay start size | 50k |\n",
    "| no-op max | 30 |\n",
    "\n",
    "---\n",
    "# Rainbow\n",
    "\n",
    "There have been many suggested improvements to the DQN architecture. In the end\n",
    "of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_\n",
    "paper combines 7 of them into a single architecture they call _Rainbow_.\n",
    "\n",
    "\n",
    "![w=38%,h=center](images/rainbow_results.png)\n",
    "\n",
    "---\n",
    "section: DDQN\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Double Q-learning\n",
    "\n",
    "Similarly to double Q-learning, instead of\n",
    "$$r + γ \\max_{a'} Q(s', a'; θ̄) - Q(s, a; θ),$$\n",
    "we minimize\n",
    "$$r + γ Q(s', \\argmax_{a'}Q(s', a'; θ); θ̄) - Q(s, a; θ).$$\n",
    "\n",
    "\n",
    "![w=30%,h=center](images/ddqn_errors.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Double Q-learning\n",
    "\n",
    "![w=100%,h=center](images/ddqn_errors_analysis.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Double Q-learning\n",
    "\n",
    "![w=60%,h=center](images/ddqn_analysis.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Double Q-learning\n",
    "\n",
    "![w=40%,h=center,mh=40%,v=middle](images/ddqn_results_5min.png)\n",
    "\n",
    "![w=55%,h=center,mh=40%,v=middle](images/ddqn_results_30min.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Prioritized Replay\n",
    "\n",
    "Instead of sampling the transitions uniformly from the replay buffer,\n",
    "we instead prefer those with a large TD error. Therefore, we sample transitions\n",
    "according to their probability\n",
    "$$p_t ∝ \\Big|r + γ \\max_{a'} Q(s', a'; θ̄) - Q(s, a; θ)\\Big|^ω,$$\n",
    "where $ω$ controls the shape of the distribution (which is uniform for $ω=0$\n",
    "and corresponds to TD error for $ω=1$).\n",
    "\n",
    "\n",
    "New transitions are inserted into the replay buffer with maximum probability\n",
    "to support exploration of all encountered transitions.\n",
    "\n",
    "---\n",
    "section: PriRep\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Prioritized Replay\n",
    "\n",
    "Because we now sample transitions according to $p_t$ instead of uniformly,\n",
    "on-policy distribution and sampling distribution differ. To compensate, we\n",
    "therefore utilize importance sampling with ratio\n",
    "$$ρ_t = \\left( \\frac{1/N}{p_t} \\right) ^β.$$\n",
    "\n",
    "\n",
    "The authors utilize in fact “for stability reasons”\n",
    "$$ρ_t / \\max_i ρ_i.$$\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Prioritized Replay\n",
    "\n",
    "![w=75%,h=center](images/prioritized_dqn_algorithm.png)\n",
    "\n",
    "---\n",
    "section: Duelling\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Duelling Networks\n",
    "\n",
    "Instead of computing directly $Q(s, a; θ)$, we compose it from the following quantities:\n",
    "- value function for a given state $s$,\n",
    "- advantage function computing an _advantage_ of using action $a$ in state $s$.\n",
    "\n",
    "$$Q(s, a) ≝ V(f(s; ζ); η) + A(f(s; ζ), a; ψ) - \\frac{\\sum_{a' ∈ 𝓐} A(f(s; ζ), a'; ψ)}{|𝓐|}$$\n",
    "\n",
    "![w=25%,h=center](images/dqn_dueling_architecture.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Duelling Networks\n",
    "\n",
    "![w=100%,h=center](images/dqn_dueling_corridor.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Duelling Networks\n",
    "\n",
    "![w=32%,h=center](images/dqn_dueling_visualization.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Duelling Networks\n",
    "\n",
    "![w=70%,h=center,mh=80%,v=middle](images/dqn_dueling_results.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "## Multi-step Learning\n",
    "\n",
    "Instead of Q-learning, we use $n$-step variant Q-learning (to be exact, we use\n",
    "$n$-step Expected Sarsa) to maximize\n",
    "$$∑_{i=1}^n γ^{i-1} r_i + γ^n \\max_{a'} Q(s', a'; θ̄) - Q(s, a; θ),$$\n",
    "\n",
    "\n",
    "This changes the off-policy algorithm to on-policy, but it is not discussed in\n",
    "any way by the authors.\n",
    "\n",
    "---\n",
    "section: NoisyNets\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Noisy Nets\n",
    "\n",
    "Noisy Nets are neural networks whose weights and biases are perturbed by\n",
    "a parametric function of a noise.\n",
    "\n",
    "\n",
    "The parameters $→θ$ are represented as\n",
    "$$→θ ≝ →μ + →σ ⊙ →ε,$$\n",
    "where $→ε$ is zero-mean noise with fixed statistics. We therefore learn the\n",
    "parameters $→ζ ≝ (→μ, →σ)$.\n",
    "\n",
    "\n",
    "Therefore, a fully connected layer\n",
    "$$→y = →w →x + →b$$\n",
    "is represented in the following way in Noisy Nets:\n",
    "$$→y = (→μ_w + →σ_w ⊙ →ε_w) →x + (→μ_b + →σ_b ⊙ →ε_b).$$\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Noisy Nets\n",
    "\n",
    "The noise $ε$ can be for example independent Gaussian noise. However, for\n",
    "performance reasons, factorized Gaussian noise is used to generate a matrix of\n",
    "noise. If $ε_{i, j}$ is noise corresponding to a layer with $i$ inputs and $j$\n",
    "outputs, we generate independent noise $ε_i$ for input neurons, independent\n",
    "noise $ε_j$ for output neurons, and set\n",
    "$$ε_{i,j} = f(ε_i) f(ε_j)$$\n",
    "for $f(x) = \\operatorname{sign}(x) \\sqrt{|x|}$.\n",
    "\n",
    "\n",
    "The authors generate noise samples for every batch, sharing the noise for all\n",
    "batch instances.\n",
    "\n",
    "\n",
    "### Deep Q Networks\n",
    "When training a DQN, $ε$-greedy is no longer used and all policies are greedy,\n",
    "and all fully connected layers are parametrized as noisy nets.\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Noisy Nets\n",
    "\n",
    "![w=50%,h=center](images/dqn_noisynets_results.png)\n",
    "\n",
    "![w=65%,h=center](images/dqn_noisynets_curves.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Noisy Nets\n",
    "\n",
    "![w=100%](images/dqn_noisynets_noise_study.png)\n",
    "\n",
    "---\n",
    "section: DistRL\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "Instead of an expected return $Q(s, a)$, we could estimate distribution of\n",
    "expected returns $Z(s, a)$.\n",
    "\n",
    "These distributions satisfy a distributional Bellman equation:\n",
    "$$Z(s, a) = R(s, a) + γ Z(s', a').$$\n",
    "\n",
    "\n",
    "The authors of the paper prove similar properties of the distributional Bellman\n",
    "operator compared to the regular Bellman operator, mainly being a contraction\n",
    "under a suitable metric (Wasserstein metric).\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "The distribution of returns is modeled as a discrete distribution parametrized\n",
    "by the number of atoms $N ∈ ℕ$ and by $V_\\textrm{MIN}, V_\\textrm{MAX} ∈ ℝ$.\n",
    "Support of the distribution are atoms\n",
    "$$\\{z_i ≝ V_\\textrm{MIN} + i Δz : 0 ≤ i < N\\}\\textrm{for~}Δz ≝ \\frac{V_\\textrm{MAX} - V_\\textrm{MIN}}{N-1}.$$\n",
    "\n",
    "\n",
    "The atom probabilities are predicted using a $\\softmax$ distribution as\n",
    "$$Z_→θ(s, a) = \\left\\{z_i\\textrm{ with probability }p_i = \\frac{e^{f_i(s, a)}}{∑_j e^{f_j(s, a)}}\\right\\}.$$\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "![w=30%,f=right](images/dqn_distributional_operator.png)\n",
    "\n",
    "After the Bellman update, the support of the distribution $R(s, a) + γZ(s', a')$\n",
    "is not the same as the original support. We therefore project it to the original\n",
    "support by proportionally mapping each atom of the Bellman update to immediate\n",
    "neighbors in the original support.\n",
    "\n",
    "\n",
    "$$Φ\\big(R(s, a) + γZ(s', a')\\big)_i ≝\n",
    "  ∑_{j=1}^N \\left[ 1 - \\frac{\\left|[r + γz_j]_{V_\\textrm{MIN}}^{V_\\textrm{MAX}}-z_i\\right|}{Δz} \\right]_0^1 p_j(s', a').$$\n",
    "\n",
    "\n",
    "The network is trained to minimize the Kullbeck-Leibler divergence between the\n",
    "current distribution and the (mapped) distribution of the one-step update\n",
    "$$D_\\textrm{KL}\\big(Φ(R + \\max_{a'} Z(s', a') || Z(s, a)\\big).$$\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "![w=50%,h=center](images/dqn_distributional_algorithm.png)\n",
    "\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "![w=40%,h=center](images/dqn_distributional_results.png)\n",
    "\n",
    "![w=40%,h=center](images/dqn_distributional_example_distribution.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "![w=100%](images/dqn_distributional_example_distributions.png)\n",
    "\n",
    "---\n",
    "# Rainbow DQN Extensions\n",
    "\n",
    "## Distributional RL\n",
    "\n",
    "![w=100%](images/dqn_distributional_atoms_ablation.png)\n",
    "\n",
    "---\n",
    "section: Rainbow\n",
    "# Rainbow Architecture\n",
    "\n",
    "Rainbow combines all described DQN extensions. Instead of $1$-step updates,\n",
    "$n$-step updates are utilized, and KL divergence of the current and target\n",
    "return distribution is minimized:\n",
    "$$D_\\textrm{KL}\\big(Φ(G_{t:t+n} + γ^n \\max_{a'} Z(s', a')) || Z(s, a)\\big).$$\n",
    "\n",
    "\n",
    "The prioritized replay chooses transitions according to the probability\n",
    "$$p_t ∝ \\Big(D_\\textrm{KL}\\big(Φ(G_{t:t+n} + γ^n \\max_{a'} Z(s', a')) || Z(s, a)\\big)\\Big)^ω.$$\n",
    "\n",
    "\n",
    "Network utilizes duelling architecture feeding the shared representation $f(s; ζ)$\n",
    "into value computation $V(f(s; ζ); η)$ and advantage computation $A_i(f(s; ζ), a; ψ)$ for atom $z_i$,\n",
    "and the final probability of atom $z_i$ in state $s$ and action $a$ is computed as\n",
    "$$p_i(s, a) ≝\n",
    "  \\frac{e^{V(f(s; ζ); η) + A_i(f(s; ζ), a; ψ) - \\sum_{a' ∈ 𝓐} A_i(f(s; ζ), a'; ψ)/|𝓐|}}\n",
    "  {\\sum_j e^{V(f(s; ζ); η) + A_j(f(s; ζ), a; ψ) - \\sum_{a' ∈ 𝓐} A_j(f(s; ζ), a'; ψ)/|𝓐|}}.$$\n",
    "\n",
    "---\n",
    "# Rainbow Hyperparameters\n",
    "\n",
    "![w=70%,h=center](images/rainbow_hyperparameters.png)\n",
    "\n",
    "---\n",
    "# Rainbow Results\n",
    "\n",
    "![w=93%,mw=50%,h=center](images/rainbow_results.png)![w=50%](images/rainbow_table.png)\n",
    "\n",
    "---\n",
    "# Rainbow Results\n",
    "\n",
    "![w=93%,mw=50%,h=center](images/rainbow_results.png)![w=93%,mw=50%,h=center](images/rainbow_results_ablations.png)\n",
    "\n",
    "---\n",
    "# Rainbow Ablations\n",
    "\n",
    "![w=90%,h=center](images/rainbow_ablations.png)\n",
    "\n",
    "---\n",
    "# Rainbow Ablations\n",
    "\n",
    "![w=84%,h=center](images/rainbow_ablations_per_game.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
