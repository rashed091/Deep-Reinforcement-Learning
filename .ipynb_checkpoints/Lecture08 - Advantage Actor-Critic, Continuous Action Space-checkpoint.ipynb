{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "section: Refresh\n",
    "# REINFORCE Algorithm\n",
    "\n",
    "The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient\n",
    "theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. To compute the gradient\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$\n",
    "REINFORCE algorithm estimates the $q_π(s, a)$ by a single sample.\n",
    "\n",
    "![w=75%,h=center](images/reinforce.png)\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "The returns can be arbitrary – better-than-average and worse-than-average\n",
    "returns cannot be recognized from the absolute value of the return.\n",
    "\n",
    "Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$\n",
    "to\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \\big(q_π(s, a) - b(s)\\big) ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of\n",
    "the estimator. Such baseline reminds centering of returns, given that\n",
    "$v_π(s) = 𝔼_{a ∼ π} q_π(s, a)$. Then, better-than-average returns are positive\n",
    "and worse-than-average returns are negative.\n",
    "\n",
    "The resulting value is also called an _advantage function_\n",
    "$a_π(s, a) ≝ q_π(s, a) - v_π(s)$.\n",
    "\n",
    "Of course, the $v_π(s)$ baseline can be only approximated. If neural networks\n",
    "are used to estimate $π(a|s; →θ)$, then some part of the network is usually\n",
    "shared between the policy and value function estimation, which is trained using\n",
    "mean square error of the predicted and observed return.\n",
    "\n",
    "---\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "![w=100%](images/reinforce_with_baseline.png)\n",
    "\n",
    "---\n",
    "# Actor-Critic\n",
    "\n",
    "It is possible to combine the policy gradient methods and temporal difference\n",
    "methods, creating a family of algorithms usually called _actor-critic_ methods.\n",
    "\n",
    "The idea is straightforward – instead of estimating the episode return using the\n",
    "whole episode rewards, we can use $n$-step temporal difference estimation.\n",
    "\n",
    "![w=64%,h=center](images/actor_critic.png)\n",
    "\n",
    "---\n",
    "section: A3C\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "A 2015 paper from Volodymyr Mnih et al., the same group as DQN.\n",
    "\n",
    "\n",
    "The authors propose an asynchronous framework, where multiple workers share one\n",
    "neural network, each training using either an off-line or on-line RL algorithm.\n",
    "\n",
    "\n",
    "They compare 1-step Q-learning, 1-step Sarsa, $n$-step Q-learning and A3C\n",
    "(an _asynchronous advantage actor-critic_ method). For A3C, they compare\n",
    "a version with and without LSTM.\n",
    "\n",
    "\n",
    "The authors also introduce _entropy regularization term_ $β H(π(s; →θ))$ to the\n",
    "loss to support exploration and discourage premature convergence.\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=45%,h=center](images/asynchronous_q_learning.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=68%,h=center](images/asynchronous_q_learning_nstep.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=68%,h=center](images/a3c.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=100%](images/a3c_performance.png)\n",
    "![w=85%,mw=50%,h=center](a3c_performance_table.png)![w=50%](a3c_speedup.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=85%,h=center](images/a3c_data_efficiency_episodes.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=85%,h=center](images/a3c_data_efficiency_time.png)\n",
    "\n",
    "---\n",
    "# Asynchronous Methods for Deep RL\n",
    "\n",
    "![w=100%,v=middle](images/a3c_learning_rates.png)\n",
    "\n",
    "---\n",
    "section: PAAC\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "An alternative to independent workers is to train in a synchronous and\n",
    "centralized way by having the workes to only generate episodes. Such approach\n",
    "was described in May 2017 by Clemente et al., who named their agent\n",
    "_parallel advantage actor-critic_ (PAAC).\n",
    "\n",
    "![w=70%,h=center](images/paac_framework.png)\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "![w=85%,h=center](images/paac_algorithm.png)\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "![w=70%,h=center](images/paac_performance.png)\n",
    "\n",
    "The authors use $8$ workers, $n_e=32$ parallel environments, $5$-step returns,\n",
    "$γ=0.99$, $ε=0.1$, $β=0.01$ and a learning rate of $α=0.0007⋅n_e=0.0224$.\n",
    "\n",
    "The $\\textrm{arch}_\\textrm{nips}$ is from A3C: 16 filters $8×8$ stride 4, 32\n",
    "filters $4×4$ stride 2, a dense layer with 256 units. The\n",
    "$\\textrm{arch}_\\textrm{nature}$ is from DQN: 32 filters $8×8$ stride 4, 64\n",
    "filters $4×4$ stride 2, 64 filters $3×3$ stride 1 and 512-unit fully connected\n",
    "layer. All nonlinearities are ReLU.\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "![w=100%](images/paac_workers_epochs.png)\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "![w=100%](images/paac_workers_time.png)\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "![w=100%,v=middle](images/paac_time_usage.png)\n",
    "\n",
    "---\n",
    "section: Continuous Action Space\n",
    "# Continuous Action Space\n",
    "\n",
    "Until now, the actions were discrete. However, many environments naturally\n",
    "accept actions from continuous space. We now consider actions which come\n",
    "from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product\n",
    "of several such ranges:\n",
    "$$∏_i [a_i, b_i].$$\n",
    "\n",
    "\n",
    "![w=40%,f=right](images/normal_distribution.png)\n",
    "A simple way how to parametrize the action distribution is to choose them from\n",
    "the normal distribution.\n",
    "\n",
    "Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$\n",
    "is\n",
    "$$p(x) ≝ \\frac{1}{\\sqrt{2 π σ^2}} e^{\\large-\\frac{(x - μ)^2}{2σ^2}}.$$\n",
    "\n",
    "---\n",
    "# Continuous Action Space in Gradient Methods\n",
    "\n",
    "Utilizing continuous action spaces in gradient-based methods is straightforward.\n",
    "Instead of the $\\softmax$ distribution we suitably parametrize the action value,\n",
    "usually using the normal distribution. Considering only one real-valued action,\n",
    "we therefore have\n",
    "$$π(a | s; →θ) ≝ P\\Big(a ∼ 𝓝\\big(μ(s; →θ), σ(s; →θ)^2\\big)\\Big),$$\n",
    "where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard\n",
    "deviation of the action distribution.\n",
    "\n",
    "The mean and standard deviation are usually computed from the shared\n",
    "representation, with\n",
    "- the mean being computed as a regular regression (i.e., one output neuron\n",
    "  without activation);\n",
    "- the standard variance (which must be positive) being computed again as\n",
    "  a regression, followed most commonly by either $\\exp$ or\n",
    "  $\\operatorname{softplus}$, where $\\operatorname{softplus}(x) ≝ \\log(1 + e^x)$.\n",
    "\n",
    "---\n",
    "# Continuous Action Space in Gradient Methods\n",
    "\n",
    "During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action\n",
    "value (clipping it to $[a, b]$ if required). To compute the loss, we utilize\n",
    "the probability density function of the normal distribution (and usually also\n",
    "add the entropy penalty).\n",
    "\n",
    "\n",
    "```python\n",
    "  mu = tf.layers.dense(hidden_layer, 1)[:, 0]\n",
    "  sd = tf.layers.dense(hidden_layer, 1)[:, 0]\n",
    "  sd = tf.exp(log_sd)   # or sd = tf.nn.softplus(sd)\n",
    "\n",
    "  normal_dist = tf.distributions.Normal(mu, sd)\n",
    "\n",
    "  # Loss computed as - log π(a|s) - entropy_regularization\n",
    "  loss = - normal_dist.log_prob(self.actions) * self.returns \\\n",
    "         - args.entropy_regularization * normal_dist.entropy()\n",
    "```\n",
    "\n",
    "---\n",
    "# Continuous Action Space\n",
    "\n",
    "When the action consists of several real values, i.e., action is a suitable\n",
    "subregion of $ℝ^n$ for $n>1$, we can:\n",
    "- either use multivariate Gaussian distribution;\n",
    "- or factorize the probability into a product of univariate normal\n",
    "  distributions.\n",
    "\n",
    "\n",
    "Modeling the action distribution using a single normal distribution might be\n",
    "insufficient, in which case a mixture of normal distributions is usually used.\n",
    "\n",
    "\n",
    "Sometimes, the continuous action space is used even for discrete output -- when\n",
    "modeling pixels intensities (256 values) or sound amplitude (2$^{16}$ values),\n",
    "instead of a softmax we use discretized mixture of distributions,\n",
    "usually $\\operatorname{logistic}$ (a distribution with a sigmoid cdf). Then,\n",
    "$$π(a) = ∑_i p_i\\Big(σ\\big((a + 0.5 - μ_i) / σ_i\\big) - σ\\big((a - 0.5 - μ_i) / σ_i\\big) \\Big).$$\n",
    "However, such mixtures are usually used in generative modeling, not in\n",
    "reinforcement learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
