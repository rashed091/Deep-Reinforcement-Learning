{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title: NPFL122, Lecture 4\n",
    "class: title, langtech, cc-by-nc-sa\n",
    "# N-step Methods, Function Approximation\n",
    "\n",
    "## Milan Straka\n",
    "\n",
    "### November 05, 2018\n",
    "\n",
    "---\n",
    "section: Refresh\n",
    "style: pre { font-size: 65% }\n",
    "# Monte Carlo Assignment\n",
    "\n",
    "```python\n",
    "Q = np.zeros([env.states, env.actions])\n",
    "C = np.zeros([env.states, env.actions])\n",
    "epsilon = args.epsilon\n",
    "evaluating = False\n",
    "\n",
    "while True:\n",
    "    # Perform episode\n",
    "    state = env.reset(evaluating)\n",
    "    states, actions, rewards = [], [], []\n",
    "    while True:\n",
    "        if evaluating or np.random.uniform() > epsilong: action = np.argmax(Q[state])\n",
    "        else:                                            action = np.random.randint(env.actions)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if env.episode >= args.episodes:\n",
    "        evaluating = True\n",
    "```\n",
    "\n",
    "---\n",
    "style: pre { font-size: 65% }\n",
    "# Monte Carlo Assignment\n",
    "\n",
    "```python\n",
    "    if not evaluating:\n",
    "        # Sum discounted rewards\n",
    "        for i in reversed(range(len(rewards) - 1)):\n",
    "            rewards[i] += args.gamma * rewards[i + 1]\n",
    "\n",
    "        # update Q and C\n",
    "        for i in range(len(rewards)):\n",
    "            C[states[i]][actions[i]] += 1\n",
    "            Q[states[i]][actions[i]] += 1 / C[states[i]][actions[i]] * (rewards[i] - Q[states[i]][actions[i]])\n",
    "\n",
    "        if args.epsilon_final:\n",
    "            epsilon = np.exp(np.interp(env.episode + 1,\n",
    "                                       [0, args.episodes],\n",
    "                                       [np.log(args.epsilon), np.log(args.epsilon_final)]))\n",
    "```\n",
    "\n",
    "---\n",
    "# Sarsa and Q-learning\n",
    "\n",
    "A straightforward application to the temporal-difference policy evaluation\n",
    "is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$\n",
    "computes\n",
    "$$q(S_t, A_t) ‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\\right].$$\n",
    "\n",
    "\n",
    "Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).\n",
    "\n",
    "$$q(S_t, A_t) ‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} +  Œ≥ \\max_a q(S_{t+1}, a) -q(S_t, A_t)\\right].$$\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Q-learning versus Sarsa\n",
    "\n",
    "![w=70%,h=center](../03/cliff_walking.pdf)\n",
    "\n",
    " ~\n",
    "# Refresh ‚Äì Q-learning versus Sarsa\n",
    "![w=40%,h=center](../03/cliff_walking.pdf)\n",
    "![w=45%,h=center](../03/cliff_walking_learning.pdf)\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Off-policy Prediction\n",
    "\n",
    "Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, ‚Ä¶, S_T$,\n",
    "the probability of this episode under a policy $œÄ$ is\n",
    "$$‚àè_{k=t}^{T-1} œÄ(A_k | S_k) p(S_{k+1} | S_k, A_k).$$\n",
    "\n",
    "\n",
    "Therefore, the relative probability of a trajectory under the target and\n",
    "behaviour policies is\n",
    "$$œÅ_t ‚âù \\frac{‚àè_{k=t}^{T-1} œÄ(A_k | S_k) p(S_{k+1} | S_k, A_k)}{‚àè_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}\n",
    "      = ‚àè_{k=t}^{T-1} \\frac{œÄ(A_k | S_k)}{b(A_k | S_k)}.$$\n",
    "\n",
    "\n",
    "Therefore, if $G_t$ is a return of episode generated according to $b$, we can\n",
    "estimate\n",
    "$$v_œÄ(S_t) = ùîº_b[œÅ_t G_t].$$\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Off-policy Monte Carlo Prediction\n",
    "\n",
    "Let $ùì£(s)$ be a set of times when we visited state $s$. Given episodes sampled\n",
    "according to $b$, we can estimate\n",
    "$$v_œÄ(s) = \\frac{‚àë_{t‚ààùì£(s)} œÅ_t G_t}{|ùì£(s)|}.$$\n",
    "\n",
    "\n",
    "Such simple average is called _ordinary importance sampling_. It is unbiased, but\n",
    "can have very high variance.\n",
    "\n",
    "\n",
    "An alternative is _weighted importance sampling_, where we compute weighted\n",
    "average as\n",
    "$$v_œÄ(s) = \\frac{‚àë_{t‚ààùì£(s)} œÅ_t G_t}{‚àë_{t‚ààùì£(s)} œÅ_t}.$$\n",
    "\n",
    "\n",
    "Weighted importance sampling is biased (with bias asymptotically converging to\n",
    "zero), but usually has smaller variance.\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Off-policy Monte Carlo Prediction\n",
    "\n",
    "![w=80%,h=center](../03/importance_sampling.pdf)\n",
    "\n",
    "Comparison of ordinary and weighted importance sampling on Blackjack. Given\n",
    "a state with sum of player's cards 13 and a usable ace, we estimate target\n",
    "policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Expected Sarsa\n",
    "\n",
    "The action $A_{t+1}$ is a source of variance, moving only _in expectation_.\n",
    "\n",
    "\n",
    "We could improve the algorithm by considering all actions proportionally to their\n",
    "policy probability, obtaining Expected Sarsa algorithm:\n",
    "$$\\begin{aligned}\n",
    "  q(S_t, A_t) &‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ ùîº_œÄ q(S_{t+1}, a) - q(S_t, A_t)\\right]\\\\\n",
    "              &‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ ‚àë\\nolimits_a œÄ(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Compared to Sarsa, the expectation removes a source of variance and therefore\n",
    "usually performs better. However, the complexity of the algorithm increases and\n",
    "becomes dependent on number of actions $|ùìê|$.\n",
    "\n",
    "\n",
    "Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour\n",
    "policy $b$ and target policy $œÄ$ to differ.\n",
    "\n",
    "\n",
    "Especially, if $œÄ$ is a greedy policy with respect to current value function,\n",
    "Expected Sarsa simplifies to Q-learning.\n",
    "\n",
    "---\n",
    "section: Double Q\n",
    "# Q-learning and Maximization Bias\n",
    "\n",
    "Because behaviour policy in Q-learning is $Œµ$-greedy variant of the target\n",
    "policy, the same samples (up to $Œµ$-greedy) determine both the maximizing action\n",
    "and estimate its value.\n",
    "\n",
    "\n",
    "![w=75%,h=center](double_q_learning_example.pdf)\n",
    "\n",
    "---\n",
    "section: Double Q\n",
    "# Double Q-learning\n",
    "\n",
    "![w=80%,h=center](double_q_learning.pdf)\n",
    "\n",
    "---\n",
    "section: $n$-step Methods\n",
    "# $n$-step Methods\n",
    "\n",
    "![w=40%,f=right](nstep_td.pdf)\n",
    "\n",
    "Full return is\n",
    "$$G_t = ‚àë_{k=t}^‚àû R_{k+1},$$\n",
    "one-step return is\n",
    "$$G_{t:t+1} = R_{t+1} + Œ≥ V(S_{t+1}).$$\n",
    "\n",
    "\n",
    "We can generalize both into $n$-step returns:\n",
    "$$G_{t:t+n} ‚âù \\left(‚àë_{k=t}^{t+n-1} Œ≥^{k-t} R_{k+1}\\right) + Œ≥^n V(S_{t+n}).$$\n",
    "with $G_{t:t+n} ‚âù G_t$ if $t+n ‚â• T$.\n",
    "\n",
    "---\n",
    "# $n$-step Methods\n",
    "\n",
    "A natural update rule is\n",
    "$$V(S_t) ‚Üê V(S_t) + Œ±\\left[G_{t:t+n} - V(S_t)\\right].$$\n",
    "\n",
    "\n",
    "![w=55%,h=center](nstep_td_prediction.pdf)\n",
    "\n",
    "---\n",
    "# $n$-step Methods Example\n",
    "\n",
    "Using the random walk example, but with 19 states instead of 5,\n",
    "![w=50%,h=center](../03/td_mc_comparison_example.pdf)\n",
    "\n",
    "we obtain the following comparison of different values of $n$:\n",
    "![w=50%,h=center](nstep_td_performance.pdf)\n",
    "\n",
    "---\n",
    "section: $n$-step Sarsa\n",
    "# $n$-step Sarsa\n",
    "\n",
    "Defining the $n$-step return to utilize action-value function as\n",
    "$$G_{t:t+n} ‚âù \\left(‚àë_{k=t}^{t+n-1} Œ≥^{k-t} R_{k+1}\\right) + Œ≥^n Q(S_{t+n}, A_{t+n})$$\n",
    "with $G_{t:t+n} ‚âù G_t$ if $t+n ‚â• T$,\n",
    "\n",
    "we get the following straightforward\n",
    "algorithm:\n",
    "$$Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±\\left[G_{t:t+n} - Q(S_t, A_t)\\right].$$\n",
    "\n",
    "\n",
    "![w=70%,h=center](nstep_sarsa_example.pdf)\n",
    "\n",
    "---\n",
    "# $n$-step Sarsa Algorithm\n",
    "\n",
    "![w=60%,h=center](nstep_sarsa_algorithm.pdf)\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Sarsa\n",
    "\n",
    "Recall the relative probability of a trajectory under the target and behaviour policies,\n",
    "which we now generalize as\n",
    "$$œÅ_{t:t+n} ‚âù ‚àè_{k=t}^{\\min(t+n, T-1)} \\frac{œÄ(A_k | S_k)}{b(A_k | S_k)}.$$\n",
    "\n",
    "\n",
    "Then a simple off-policy $n$-step TD can be computed as\n",
    "$$V(S_t) ‚Üê V(S_t) + Œ±œÅ_{t:t+n-1}\\left[G_{t:t+n} - V(S_t)\\right].$$\n",
    "\n",
    "\n",
    "Similarly, $n$-step Sarsa becomes\n",
    "$$Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±œÅ_{\\boldsymbol{t+1}:\\boldsymbol{t+n}}\\left[G_{t:t+n} - Q(S_t, A_t)\\right].$$\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Sarsa\n",
    "\n",
    "![w=60%,h=center](off_policy_nstep_sarsa.pdf)\n",
    "\n",
    "---\n",
    "section: Tree Backup\n",
    "# Off-policy $n$-step Without Importance Sampling\n",
    "\n",
    "![w=30%,h=center](off_policy_nstep_algorithms.pdf)\n",
    "\n",
    "Q-learning and Expected Sarsa can learn off-policy without importance sampling.\n",
    "\n",
    "\n",
    "To generalize to $n$-step off-policy method, we must compute expectations\n",
    "over actions in each step of $n$-step update. However, we have not obtained\n",
    "a return for the non-sampled actions.\n",
    "\n",
    "\n",
    "Luckily, we can estimate their values by using the current action-value\n",
    "function.\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Without Importance Sampling\n",
    "\n",
    "![w=10%,f=right](tree_backup_example.pdf)\n",
    "\n",
    "\n",
    "We now derive the $n$-step reward, starting from one-step:\n",
    "$$G_{t:t+1} ‚âù R_{t+1} + ‚àë\\nolimits_a œÄ(a|S_{t+1}) Q(S_{t+1}, a).$$\n",
    "\n",
    "\n",
    "For two-step, we get:\n",
    "$$G_{t:t+2} ‚âù R_{t+1} + Œ≥‚àë\\nolimits_{a‚â†A_{t+1}} œÄ(a|S_{t+1}) Q(S_{t+1}, a) + Œ≥œÄ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$\n",
    "\n",
    "\n",
    "Therefore, we can generalize to:\n",
    "$$G_{t:t+n} ‚âù R_{t+1} + Œ≥‚àë\\nolimits_{a‚â†A_{t+1}} œÄ(a|S_{t+1}) Q(S_{t+1}, a) + Œ≥œÄ(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Without Importance Sampling\n",
    "\n",
    "![w=55%,h=center](tree_backup_algorithm.pdf)\n",
    "\n",
    "---\n",
    "section: Function Approximation\n",
    "# Function Approximation\n",
    "\n",
    "We will approximate value function $v$ and/or state-value function $q$, choosing\n",
    "from a family of functions parametrized by a weight vector $‚Üíw‚àà‚Ñù^d$.\n",
    "\n",
    "\n",
    "We will denote the approximations as\n",
    "$$\\begin{gathered}\n",
    "  vÃÇ(s, ‚Üíw),\\\\\n",
    "  qÃÇ(s, a, ‚Üíw).\n",
    "\\end{gathered}$$\n",
    "\n",
    "\n",
    "\n",
    "Weights are usually shared among states. Therefore, we need to define state\n",
    "distribution $Œº(s)$ to allow an objective for finding the best function approximation.\n",
    "\n",
    "\n",
    "The state distribution $Œº(s)$ gives rise to a natural objective function called\n",
    "_Mean Squared Value Error_, denoted $\\overline{VE}$:\n",
    "$$\\overline{VE}(‚Üíw) ‚âù ‚àë_{s‚ààùì¢} Œº(s) \\left[v_œÄ(s) - vÃÇ(s, ‚Üíw)\\right]^2.$$\n",
    "\n",
    "---\n",
    "# Function Approximation\n",
    "\n",
    "For on-policy algorithms, $Œº$ is usually on-policy distribution. That is the\n",
    "stationary distribution under $œÄ$ for continuous tasks, and for the episodic\n",
    "case it is defined as\n",
    "$$\\begin{aligned}\n",
    "  Œ∑(s) &= h(s) + ‚àë_{s'}Œ∑(s')‚àë_a œÄ(a|s') p(s|s', a),\\\\\n",
    "  Œº(s) &= \\frac{Œ∑(s)}{‚àë_{s'} Œ∑(s')},\n",
    "\\end{aligned}$$\n",
    "where $h(s)$ is a probability that an episodes starts in state $s$.\n",
    "\n",
    "---\n",
    "# Gradient and Semi-Gradient Methods\n",
    "\n",
    "The functional approximation (i.e., the weight vector $‚Üíw$) is usually optimized\n",
    "using gradient methods, for example as\n",
    "$$\\begin{aligned}\n",
    "  ‚Üíw_{t+1} &‚Üê ‚Üíw_t - \\frac{1}{2} Œ± ‚àá \\left[v_œÄ(S_t) - vÃÇ(S_t, ‚Üíw_t)\\right]^2\\\\\n",
    "           &‚Üê ‚Üíw_t + Œ±\\left[v_œÄ(S_t) - vÃÇ(S_t, ‚Üíw_t)\\right] ‚àá vÃÇ(S_t, ‚Üíw_t).\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "As usual, the $v_œÄ(S_t)$ is estimated by a suitable sample. For example in Monte\n",
    "Carlo methods, we use episodic return $G_t$, and in temporal difference methods,\n",
    "we employ bootstrapping and use $R_{t+1} + Œ≥vÃÇ(S_{t+1}, ‚Üíw).$\n",
    "\n",
    "---\n",
    "# Monte Carlo Gradient Policy Evaluation\n",
    "![w=100%,v=middle](grad_mc_estimation.pdf)\n",
    "\n",
    "---\n",
    "# Linear Methods\n",
    "\n",
    "A simple special case of function approximation are linear methods, where\n",
    "$$vÃÇ(‚Üíx(s), ‚Üíw) ‚âù ‚Üíx(s)^T ‚Üíw = ‚àëx(s)_i w_i.$$\n",
    "\n",
    "\n",
    "The $‚Üíx(s)$ is a representation of state $s$, which is a vector of the same size\n",
    "as $‚Üíw$. It is sometimes called a _feature vector_.\n",
    "\n",
    "\n",
    "The SGD update rule then becomes\n",
    "$$‚Üíw_{t+1} ‚Üê ‚Üíw_t + Œ±\\left[v_œÄ(S_t) - vÃÇ(‚Üíx(S_t), ‚Üíw_t)\\right] ‚Üíx(S_t).$$\n",
    "\n",
    "---\n",
    "# State Aggregation\n",
    "\n",
    "Simple way of generating a feature vector is _state aggregation_, where several\n",
    "neighboring states are grouped together.\n",
    "\n",
    "\n",
    "For example, consider a 1000-state random walk, where transitions go uniformly\n",
    "randomly to any of 100 neighboring states on the left or on the right. Using\n",
    "state aggregation, we can partition the 1000 states into 10 groups of 100\n",
    "states. Monte Carlo policy evaluation then computes the following:\n",
    "\n",
    "![w=60%,h=center](grad_mc_estimation_example.pdf)\n",
    "\n",
    "---\n",
    "# Feature Construction for Linear Methods\n",
    "\n",
    "Many methods developed in the past:\n",
    "\n",
    "\n",
    "- polynomials\n",
    "\n",
    "\n",
    "- Fourier basis\n",
    "\n",
    "\n",
    "- tile coding\n",
    "\n",
    "\n",
    "- radial basis functions\n",
    "\n",
    "\n",
    "But of course, nowadays we use deep neural networks which construct a suitable\n",
    "feature vector automatically as a latent variable (the last hidden layer).\n",
    "\n",
    "---\n",
    "# Tile Coding\n",
    "\n",
    "![w=100%,mh=90%,v=middle](tile_coding.pdf)\n",
    "\n",
    "\n",
    "If $t$ overlapping tiles are used, the learning rate is usually normalized as $Œ±/t$.\n",
    "\n",
    "---\n",
    "# Tile Coding\n",
    "\n",
    "For example, on the 1000-state random walk example, the performance of tile\n",
    "coding surpasses state aggregation:\n",
    "\n",
    "![w=60%,h=center](tile_coding_performance.pdf)\n",
    "\n",
    "---\n",
    "# Asymmetrical Tile Coding\n",
    "\n",
    "In higher dimensions, the tiles should have asymmetrical offsets, with\n",
    "a sequence of $(1, 3, 5, ‚Ä¶, 2d-1)$ being a good choice.\n",
    "\n",
    "![w=50%,h=center](tile_coding_asymmetrical.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
