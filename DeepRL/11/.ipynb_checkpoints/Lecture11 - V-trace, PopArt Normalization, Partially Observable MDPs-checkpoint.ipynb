{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title: NPFL122, Lecture 11\n",
    "class: title, langtech, cc-by-nc-sa\n",
    "# V-trace, PopArt Normalization, Partially Observable MDPs\n",
    "\n",
    "## Milan Straka\n",
    "\n",
    "### January 7, 2019\n",
    "\n",
    "---\n",
    "section: IMPALA\n",
    "# IMPALA\n",
    "\n",
    "Impala (**Imp**ortance Weighted **A**ctor-**L**earner **A**rchitecture) was\n",
    "suggested in Feb 2018 paper and allows massively distributed implementation\n",
    "of an actor-critic-like learning algorithm.\n",
    "\n",
    "\n",
    "Compared to A3C-based agents, which communicates gradients with respect to the\n",
    "parameters of the policy, IMPALA actors communicates trajectories to the\n",
    "centralized learner.\n",
    "\n",
    "\n",
    "![w=50%](impala_overview.pdf)\n",
    " ~~\n",
    "![w=50%](impala_overview.pdf)![w=50%](impala_comparison.pdf)\n",
    "\n",
    "\n",
    "If many actors are used, the policy used to generate a trajectory can lag behind\n",
    "the latest policy. Therefore, a new **V-trace** off-policy actor-critic\n",
    "algorithm is proposed.\n",
    "\n",
    "---\n",
    "# IMPALA â€“ V-trace\n",
    "\n",
    "Consider a trajectory $(S_t, A_t, R_{t+1})_{t=s}^{t=s+n}$ generated by\n",
    "a behaviour policy $b$.\n",
    "\n",
    "\n",
    "The $n$-step V-trace target for $S_s$ is defined as\n",
    "$$v_s â‰ V(S_s) + âˆ‘_{t=s}^{s+n-1} Î³^{t-s} \\left(âˆ\\nolimits_{i=s}^{t-1} c_i\\right) Î´_t V,$$\n",
    "\n",
    "where $Î´_t V$ is the temporal difference for V\n",
    "$$Î´_t V â‰ Ï_t \\big(R_{t+1} + Î³V(s_{t+1}) - V(s_t)\\big),$$\n",
    "\n",
    "and $Ï_t$ and $c_i$ are truncated importance sampling ratios with $ÏÌ„ â‰¥ cÌ„$:\n",
    "$$Ï_t â‰ \\min\\left(ÏÌ„, \\frac{Ï€(A_t | S_t)}{b(A_t | S_t)}\\right),~c_i â‰ \\min\\left(cÌ„, \\frac{Ï€(A_i | S_i)}{b(A_i | S_i)}\\right).$$\n",
    "\n",
    "\n",
    "Note that if $b=Ï€$ and assuming $cÌ„ â‰¥ 1$, $v_s$ reduces to $n$-step Bellman\n",
    "target.\n",
    "\n",
    "---\n",
    "# IMPALA â€“ V-trace\n",
    "\n",
    "Note that the truncated IS weights $Ï_t$ and $c_i$ play different roles:\n",
    "\n",
    "\n",
    "- The $Ï_t$ appears in the definition of $Î´_t V$ and defines the fixed point\n",
    "  of the update rule. For $ÏÌ„=âˆ$, the target is the value function $v_Ï€$,\n",
    "  if $ÏÌ„<âˆ$, the fixed point is somewhere between $v_Ï€$ and $v_b$. Notice that\n",
    "  we do not compute a product of these $Ï_t$ coefficients.\n",
    "\n",
    "- The $c_i$ impacts the speed of convergence (the contraction rate of the\n",
    "  Bellman operator), not the sought policy. Because a product of the $c_i$\n",
    "  ratios is computed, it plays an important role in variance reduction.\n",
    "\n",
    "\n",
    "The paper utilizes $cÌ„=1$ and out of $ÏÌ„ âˆˆ \\{1, 10, 100\\}$, $ÏÌ„=1$ works\n",
    "empirically the best.\n",
    "\n",
    "---\n",
    "# IMPALA â€“ V-trace\n",
    "\n",
    "Consider a parametrized functions computing $v(s; â†’Î¸)$ and $Ï€(a|s; â†’Ï‰)$.\n",
    "Assuming the defined $n$-step V-trace target\n",
    "$$v_s â‰ V(S_s) + âˆ‘_{t=s}^{s+n-1} Î³^{t-s} \\left(âˆ\\nolimits_{i=s}^{t-1} c_i\\right) Î´_t V,$$\n",
    "\n",
    "\n",
    "we update the critic in the direction of\n",
    "$$\\big(v_s - v(S_s; â†’Î¸)\\big) âˆ‡_â†’Î¸ v(S_s; â†’Î¸)$$\n",
    "\n",
    "\n",
    "and the actor in the direction of the policy gradient\n",
    "$$Ï_s âˆ‡_â†’Ï‰ \\log Ï€(A_s | S_s; â†’Ï‰)\\big(R_{s+1} + Î³ v_{s+1} - v(S_s; â†’Î¸)\\big).$$\n",
    "\n",
    "\n",
    "Finally, we again add the entropy regularization term $H(Ï€(â‹… | S_s; â†’Î¸))$ to the\n",
    "loss function.\n",
    "\n",
    "---\n",
    "# IMPALA\n",
    "\n",
    "![w=60%,h=center](impala_throughput.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Population Based Training\n",
    "\n",
    "For Atari experiments, population based training with a population of 24 agents\n",
    "is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the\n",
    "global gradient norm clipping threshold.\n",
    "\n",
    "\n",
    "![w=80%,h=center](pbt_overview.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Population Based Training\n",
    "\n",
    "For Atari experiments, population based training with a population of 24 agents\n",
    "is used to adapt entropy regularization, learning rate, RMSProp $Îµ$ and the\n",
    "global gradient norm clipping threshold.\n",
    "\n",
    "In population based training, several agents are trained in parallel. When an\n",
    "agent is _ready_ (after 5000 episodes), then:\n",
    "\n",
    "- it may be overwritten by parameters and hyperparameters of another agent, if\n",
    "  it is sufficiently better (5000 episode mean capped human normalized score returns\n",
    "  are 5% better);\n",
    "\n",
    "- and independently, the hyperparameters may undergo a change (multiplied by\n",
    "  either 1.2 or 1/1.2 with 33% chance).\n",
    "\n",
    "\n",
    "---\n",
    "# IMPALA\n",
    "\n",
    "![w=100%,v=middle](impala_results.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Learning Curves\n",
    "\n",
    "![w=32%,h=center](impala_curves.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Atari Games\n",
    "\n",
    "![w=60%,h=center,v=middle](impala_results_atari.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Ablations\n",
    "\n",
    "![w=60%,h=center](impala_ablations_table.pdf)\n",
    "\n",
    "---\n",
    "# IMPALA â€“ Ablations\n",
    "\n",
    "![w=50%,h=center](impala_ablations_graphs.pdf)\n",
    "\n",
    "---\n",
    "section: PopArt Normalization\n",
    "# PopArt Normalization\n",
    "\n",
    "An improvement of IMPALA from Sep 2018, which performs normalization of task\n",
    "rewards instead of just reward clipping. PopArt stands for _Preserving Outputs\n",
    "Precisely, while Adaptively Rescaling Targets_.\n",
    "\n",
    "\n",
    "Assume the value estimate $v(s; â†’Î¸, Ïƒ, Î¼)$ is computed using a normalized value\n",
    "predictor $n(s; â†’Î¸)$\n",
    "$$v(s; â†’Î¸, Ïƒ, Î¼) â‰ Ïƒ n(s; â†’Î¸) + Î¼$$\n",
    "and further assume that $n(s; â†’Î¸)$ is an output of a linear function\n",
    "$$n(s; â†’Î¸) â‰ â†’Ï‰^T f(s; â†’Î¸-\\{â†’Ï‰, b\\}) + b.$$\n",
    "\n",
    "\n",
    "We can update the $Ïƒ$ and $Î¼$ using exponentially moving average with decay rate\n",
    "$Î²$ (in the paper, first moment $Î¼$ and second moment $Ï…$ is tracked, and\n",
    "standard deviation is computed as $Ïƒ=\\sqrt{Ï…-Î¼^2}$; decay rate $Î²=3 â‹… 10^{-4}$ is employed).\n",
    "\n",
    "---\n",
    "# PopArt Normalization\n",
    "\n",
    "Utilizing the parameters $Î¼$ and $Ïƒ$, we can normalize the observed (unnormalized) returns as\n",
    "$(G - Î¼) / Ïƒ$ and use an actor-critic algorithm with advantage $(G - Î¼)/Ïƒ - n(S; â†’Î¸)$.\n",
    "\n",
    "\n",
    "However, in order to make sure the value function estimate does not change when\n",
    "the normalization parameters change, the parameters $â†’Ï‰, b$ computing the\n",
    "unnormalized value estimate are updated under any change $Î¼ â†’ Î¼'$ and $Ïƒ â†’ Ïƒ'$ as:\n",
    "$$â†’Ï‰' â‰ \\frac{Ïƒ}{Ïƒ'}â†’Ï‰,~b' â‰ \\frac{Ïƒb + Î¼ - Î¼'}{Ïƒ'}.$$\n",
    "\n",
    "\n",
    "In multi-task settings, we train a task-agnostic policy and task-specific value\n",
    "functions (therefore, $â†’Î¼$, $â†’Ïƒ$ and $â†’n(s; â†’Î¸)$ are vectors).\n",
    "\n",
    "---\n",
    "# PopArt Results\n",
    "\n",
    "![w=80%,h=center](popart_results.pdf)\n",
    "\n",
    "\n",
    "![w=100%](popart_atari_curves.pdf)\n",
    "\n",
    "---\n",
    "# PopArt Results\n",
    "\n",
    "![w=90%,h=center](popart_atari_statistics.pdf)\n",
    "\n",
    "---\n",
    "# PopArt Results\n",
    "\n",
    "![w=100%,v=middle](popart_dmlab_curves.pdf)\n",
    "\n",
    "---\n",
    "section: POMDPs\n",
    "# Partially Observable MDPs\n",
    "\n",
    "Recall that a _Markov decision process_ (MDP) is a quadruple $(ğ“¢, ğ“, p, Î³)$,\n",
    "where:\n",
    "- $ğ“¢$ is a set of states,\n",
    "- $ğ“$ is a set of actions,\n",
    "- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that\n",
    "  action $a âˆˆ ğ“$ will lead from state $s âˆˆ ğ“¢$ to $s' âˆˆ ğ“¢$, producing a _reward_ $r âˆˆ â„$,\n",
    "- $Î³ âˆˆ [0, 1]$ is a _discount factor_.\n",
    "\n",
    "\n",
    "_Partially observable Markov decision process_ extends the Markov decision\n",
    "process to a sextuple $(ğ“¢, ğ“, p, Î³, ğ“, o)$, where in addition to an MDP\n",
    "- $ğ“$ is a set of observations,\n",
    "- $o(O_t | S_t, A_{t-1})$ is an observation model.\n",
    "\n",
    "\n",
    "In robotics (out of the domain of this course), several approaches are used to\n",
    "handle POMDPs, to model uncertainty, imprecise mechanisms and inaccurate\n",
    "sensors.\n",
    "\n",
    "---\n",
    "# Partially Observable MDPs\n",
    "\n",
    "In Deep RL, partially observable MDPs are usually handled using recurrent\n",
    "networks. After suitable encoding of input observation $O_t$ and previous\n",
    "action $A_{t-1}$, a RNN (usually LSTM) unit is used to model the current $S_t$\n",
    "(or its suitable latent representation), which is in turn utilized to produce\n",
    "$A_t$.\n",
    "\n",
    "![w=50%,h=center](merlin_rl-lstm.pdf)\n",
    "\n",
    "---\n",
    "section: MERLIN\n",
    "# MERLIN\n",
    "\n",
    "However, keeping all information in the RNN state is substantially limiting.\n",
    "Therefore, _memory-augmented_ networks can be used to store suitable information\n",
    "in external memory (in the lines of NTM, DNC or MANN models).\n",
    "\n",
    "We now describe an approach used by Merlin architecture (_Unsupervised\n",
    "Predictive Memory in a Goal-Directed Agent_ DeepMind Mar 2018 paper).\n",
    "\n",
    "![w=50%,h=center](merlin_rl-mem.pdf)\n",
    "\n",
    "---\n",
    "# MERLIN â€“ Memory Module\n",
    "\n",
    "![w=30%,f=right](merlin_rl-mem.pdf)\n",
    "\n",
    "Let $â†’M$ be a memory matrix of size $N_\\textit{mem} Ã— 2|z|$.\n",
    "\n",
    "\n",
    "Assume we have already encoded observations as $â†’e_t$ and previous action\n",
    "$a_{t-1}$. We concatenate them with $K$ previously read vectors and process\n",
    "by a deep LSTM (two layers are used in the paper) to compute $â†’h_t$.\n",
    "\n",
    "\n",
    "Then, we apply a linear layer to $â†’h_t$, computing $K$ key vectors\n",
    "$â†’k_1, â€¦ â†’k_K$ of length $2|z|$ and $K$ positive scalars $Î²_1, â€¦, Î²_K$.\n",
    "\n",
    "\n",
    "**Reading:** For each $i$, we compute cosine similarity of $â†’k_i$ and all memory\n",
    "rows $M_j$, multiply the similarities by $Î²_i$ and pass them through a $\\softmax$\n",
    "to obtain weights $â†’Ï‰_i$. The read vector is then computed as $â‡‰M â†’w_i$.\n",
    "\n",
    "\n",
    "**Writing:** We find one-hot write index $â†’v_\\textit{wr}$ to be the least used\n",
    "memory row (we keep usage indicators and add read weights to them). We then\n",
    "compute $â†’v_\\textit{ret} â† Î³ â†’v_\\textit{ret} + (1 - Î³) â†’v_\\textit{wr}$, and update\n",
    "the memory matrix using $â‡‰M â† â‡‰M + â†’v_\\textit{wr}[â†’e_t, 0] + â†’v_\\textit{ret}[0, â†’e_t]$.\n",
    "\n",
    "---\n",
    "# MERLIN â€” Prior and Posterior\n",
    "\n",
    "However, updating the encoder and memory content purely using RL is inefficient.\n",
    "Therfore, MERLIN includes a _memory-based predictor (MBP)_ in addition to policy.\n",
    "The goal of MBP is to compress observations into low-dimensional state\n",
    "representations $z$ and storing them in memory.\n",
    "\n",
    "\n",
    "According to the paper, the idea of unsupervised and predictive modeling has\n",
    "been entertained for decades, and recent discussions have proposed such modeling\n",
    "to be connected to hippocampal memory.\n",
    "\n",
    "We want the state variables not only to faithfully represent the data, but also\n",
    "emphasise rewarding elements of the environment above irrelevant ones. To\n",
    "accomplish this, the authors follow the hippocampal representation theory of\n",
    "Gluck and Myers, who proposed that hippocampal representations pass through\n",
    "a compressive bottleneck and then reconstruct input stimuli together with task\n",
    "reward.\n",
    "\n",
    "\n",
    "In MERLIN, a _prior_ distribution over $z_t$ predicts next state variable\n",
    "conditioned on history of state variables and actions $p(z_t | z_{t-1}, a_{t-1}, â€¦, z_1, a_1)$,\n",
    "and _posterior_ corrects the prior using the new observation $o_t$, forming\n",
    "a better estimate $q(z_t | o_t, z_{t-1}, a_{t-1}, â€¦, z_1, a_1)$.\n",
    "\n",
    "---\n",
    "# MERLIN â€” Prior and Posterior\n",
    "\n",
    "To achieve the mentioned goals, we add two terms to the loss.\n",
    "\n",
    "- We try reconstructing input stimuli, action, reward and return using a sample from\n",
    "  the state variable posterior, and add the difference of the reconstruction and\n",
    "  ground truth to the loss.\n",
    "\n",
    "\n",
    "- We also add KL divergence of the prior and posterior to the loss, to ensure\n",
    "  consistency between the prior and posterior.\n",
    "\n",
    "\n",
    "![w=85%,h=center](merlin_diagram.pdf)\n",
    "\n",
    "---\n",
    "# MERLIN â€” Algorithm\n",
    "\n",
    "![w=37%,h=center](merlin_algorithm.pdf)\n",
    "\n",
    "---\n",
    "# MERLIN\n",
    "\n",
    "![w=70%,h=center](merlin_tasks.pdf)\n",
    "\n",
    "---\n",
    "# MERLIN\n",
    "\n",
    "![w=50%,h=center](merlin_analysis.pdf)\n",
    "\n",
    "---\n",
    "# MERLIN\n",
    "\n",
    "![w=90%,h=center](merlin_predictive_model.pdf)\n",
    "\n",
    "---\n",
    "section: CTF-FTW\n",
    "# For the Win agent for Capture The Flag\n",
    "\n",
    "![w=100%](ctf_overview.pdf)\n",
    "\n",
    "---\n",
    "# For the Win agent for Capture The Flag\n",
    "\n",
    "- Extension of the MERLIN architecture.\n",
    "\n",
    "\n",
    "- Hierarchical RNN with two timescales.\n",
    "\n",
    "\n",
    "- Population based training controlling KL divergence penalty weights,\n",
    "  slow ticking RNN speed and gradient flow factor from fast to slow RNN.\n",
    "\n",
    "---\n",
    "# For the Win agent for Capture The Flag\n",
    "\n",
    "![w=47%,h=center](ctf_architecture.pdf)\n",
    "\n",
    "---\n",
    "# For the Win agent for Capture The Flag\n",
    "\n",
    "![w=80%,h=center](ctf_curves.pdf)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
