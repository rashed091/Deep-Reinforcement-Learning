{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refresh ‚Äì Policies and Value Functions\n",
    "\n",
    "A _policy_ $œÄ$ computes a distribution of actions in a given state, i.e.,\n",
    "$œÄ(a | s)$ corresponds to a probability of performing an action $a$ in state\n",
    "$s$.\n",
    "\n",
    "\n",
    "To evaluate a quality of a policy, we define _value function_ $v_œÄ(s)$, or\n",
    "_state-value function_, as\n",
    "$$v_œÄ(s) ‚âù ùîº_œÄ\\left[G_t \\middle| S_t = s\\right] = ùîº_œÄ\\left[‚àë\\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \\middle| S_t=s\\right].$$\n",
    "\n",
    "\n",
    "An _action-value function_ for a policy $œÄ$ is defined analogously as\n",
    "$$q_œÄ(s, a) ‚âù ùîº_œÄ\\left[G_t \\middle| S_t = s, A_t = a\\right] = ùîº_œÄ\\left[‚àë\\nolimits_{k=0}^‚àû Œ≥^k R_{t+k+1} \\middle| S_t=s, A_t = a\\right].$$\n",
    "\n",
    "\n",
    "Optimal state-value function is defined as $v_*(s) ‚âù \\max_œÄ v_œÄ(s),$\n",
    "analogously optimal action-value function is defined as $q_*(s, a) ‚âù \\max_œÄ q_œÄ(s, a).$\n",
    "\n",
    "Any policy $œÄ_*$ with $v_{œÄ_*} = v_*$ is called an _optimal policy_.\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Value Iteration\n",
    "\n",
    "Optimal value function can be computed by repetitive application of Bellman\n",
    "optimality equation:\n",
    "$$\\begin{aligned}\n",
    "v_0(s) &‚Üê 0 \\\\\n",
    "v_{k+1}(s) &‚Üê \\max_a ùîº\\left[R_{t+1} + Œ≥ v_k(S_{t+1}) \\middle| S_t=s, A_t=a\\right] = B v_k.\n",
    "\\end{aligned}$$\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Policy Iteration Algorithm\n",
    "\n",
    "Policy iteration consists of repeatedly performing policy evaluation and policy\n",
    "improvement:\n",
    "$$œÄ_0 \\stackrel{E}{\\longrightarrow} v_{œÄ_0} \\stackrel{I}{\\longrightarrow}\n",
    "  œÄ_1 \\stackrel{E}{\\longrightarrow} v_{œÄ_1} \\stackrel{I}{\\longrightarrow}\n",
    "  œÄ_2 \\stackrel{E}{\\longrightarrow} v_{œÄ_2} \\stackrel{I}{\\longrightarrow}\n",
    "  ‚Ä¶ \\stackrel{I}{\\longrightarrow} œÄ_* \\stackrel{E}{\\longrightarrow} v_{œÄ_*}.$$\n",
    "\n",
    "\n",
    "The result is a sequence of monotonically improving policies $œÄ_i$. Note that\n",
    "when $œÄ' = œÄ$, also $v_{œÄ'} = v_œÄ$, which means Bellman optimality equation is\n",
    "fulfilled and both $v_œÄ$ and $œÄ$ are optimal.\n",
    "\n",
    "\n",
    "Considering that there is only a finite number of policies, the optimal policy\n",
    "and optimal value function can be computed in finite time (contrary to value\n",
    "iteration, where the convergence is only asymptotic).\n",
    "\n",
    "\n",
    "Note that when evaluation policy $œÄ_{k+1}$, we usually start with $v_{œÄ_k}$,\n",
    "which is assumed to be a good approximation to $v_{œÄ_{k+1}}$.\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Generalized Policy Iteration\n",
    "\n",
    "_Generalized Policy Evaluation_ is a general idea of interleaving policy\n",
    "evaluation and policy improvement at various granularity.\n",
    "\n",
    "![w=30%,mw=50%,h=center](../02/gpi.pdf)![w=80%,mw=50%,h=center](../02/gpi_convergence.pdf)\n",
    "\n",
    "If both processes stabilize, we know we have obtained optimal policy.\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì $Œµ$-soft Policies\n",
    "\n",
    "A policy is called $Œµ$-soft, if\n",
    "$$œÄ(a|s) ‚â• \\frac{Œµ}{|ùìê(s)|}.$$\n",
    "\n",
    "\n",
    "We call a policy $Œµ$-greedy, if one action has maximum probability of\n",
    "$1-Œµ+\\frac{Œµ}{|A(s)|}$.\n",
    "\n",
    "\n",
    "The policy improvement theorem can be proved also for class of $Œµ$-soft\n",
    "policies, and using<br>$Œµ$-greedy policy in policy improvement step, policy\n",
    "iteration has same convergence properties. (We can embed the $Œµ$-soft behaviour\n",
    "‚Äúinside‚Äù the environment and prove equivalence.)\n",
    "\n",
    "---\n",
    "# Refresh ‚Äì Monte Carlo for $Œµ$-soft Policies\n",
    "\n",
    "### On-policy every-visit Monte Carlo for $Œµ$-soft Policies\n",
    "Algorithm parameter: small $Œµ>0$\n",
    "\n",
    "Initialize $Q(s, a) ‚àà ‚Ñù$ arbitrarily (usually to 0), for all $s ‚àà ùì¢, a ‚àà ùìê$<br>\n",
    "Initialize $C(s, a) ‚àà ‚Ñ§$ to 0, for all $s ‚àà ùì¢, a ‚àà ùìê$\n",
    "\n",
    "Repeat forever (for each episode):\n",
    "- Generate an episode $S_0, A_0, R_1, ‚Ä¶, S_{T-1}, A_{T-1}, R_T$,\n",
    "  by generating actions as follows:\n",
    "  - With probability $Œµ$, generate a random uniform action\n",
    "  - Otherwise, set $A_t ‚âù \\argmax\\nolimits_a Q(S_t, a)$\n",
    "- $G ‚Üê 0$\n",
    "- For each $t=T-1, T-2, ‚Ä¶, 0$:\n",
    "  - $G ‚Üê Œ≥G + R_{T+1}$\n",
    "  - $C(S_t, A_t) ‚Üê C(S_t, A_t) + 1$\n",
    "  - $Q(S_t, A_t) ‚Üê Q(S_t, A_t) + \\frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$\n",
    "\n",
    "---\n",
    "section: Afterstates\n",
    "# Action-values and Afterstates\n",
    "\n",
    "The reason we estimate _action-value_ function $q$ is that the policy is\n",
    "defined as\n",
    "$$\\begin{aligned}\n",
    "  œÄ(s) &‚âù \\argmax_a q_œÄ(s, a) \\\\\n",
    "       &= \\argmax_a ‚àë\\nolimits_{s', r} p(s', r | s, a) \\left[r + Œ≥ v_œÄ(s')\\right]\n",
    "\\end{aligned}$$\n",
    "and the latter form might be impossible to evaluate if we do not have the model\n",
    "of the environment.\n",
    "\n",
    "\n",
    "![w=80%,mw=40%,h=center,f=right](afterstates.pdf)\n",
    "However, if the environment is known, it might be better to estimate returns only\n",
    "for states, and there can be substantially less states than state-action pairs.\n",
    "\n",
    "---\n",
    "section: TD\n",
    "# TD Methods\n",
    "\n",
    "Temporal-difference methods estimate action-value returns using one iteration of\n",
    "Bellman equation instead of complete episode return.\n",
    "\n",
    "\n",
    "Compared to Monte Carlo method with constant learning rate $Œ±$, which performs\n",
    "$$v(S_t) ‚Üê v(S_t) + Œ±\\left[G_t - v(S_t)\\right],$$\n",
    "the simplest temporal-difference method computes the following:\n",
    "$$v(S_t) ‚Üê v(S_t) + Œ±\\left[R_{t+1} + Œ≥v(S_{t+1}) - v(S_t)\\right],$$\n",
    "\n",
    "---\n",
    "# TD Methods\n",
    "\n",
    "![w=70%,h=center](td_example.pdf)\n",
    "\n",
    "\n",
    "![w=70%,h=center](td_example_update.pdf)\n",
    "\n",
    "---\n",
    "# TD and MC Comparison\n",
    "\n",
    "As with Monte Carlo methods, for a fixed policy $œÄ$, TD methods converge to\n",
    "$v_œÄ$.\n",
    "\n",
    "\n",
    "On stochastic tasks, TD methods usually converge to $v_œÄ$ faster than constant-$Œ±$ MC\n",
    "methods.\n",
    "\n",
    "\n",
    "![w=70%,h=center](td_mc_comparison_example.pdf)\n",
    "\n",
    "\n",
    "![w=75%,h=center](td_mc_comparison.pdf)\n",
    "\n",
    "---\n",
    "# Optimality of MC and TD Methods\n",
    "\n",
    "![w=70%,mw=50%,h=center](td_mc_optimality_example.pdf)![w=90%,mw=50%,h=center](td_mc_optimality_data.pdf)\n",
    "\n",
    "\n",
    "For state B, 6 out of 8 times return from B was 1 and 0 otherwise.\n",
    "Therefore, $v(B) = 3/4$.\n",
    "\n",
    "\n",
    "- [TD] For state A, in all cases it transfered to B. Therefore, $v(A)$ could be $3/4$.\n",
    "\n",
    "- [MC] For state A, in all cases it generated return 0. Therefore, $v(A)$ could be $0$.\n",
    "\n",
    "\n",
    "MC minimizes error on training data, TD minimizes MLE error for the Markov\n",
    "process.\n",
    "\n",
    "---\n",
    "# Sarsa\n",
    "\n",
    "A straightforward application to the temporal-difference policy evaluation\n",
    "is Sarsa algorithm, which after generating $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$\n",
    "computes\n",
    "$$q(S_t, A_t) ‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ q(S_{t+1}, A_{t+1}) -q(S_t, A_t)\\right].$$\n",
    "\n",
    "\n",
    "![w=75%,h=center](sarsa.pdf)\n",
    "\n",
    "---\n",
    "# Sarsa\n",
    "\n",
    "![w=65%,h=center](sarsa_example.pdf)\n",
    "\n",
    "\n",
    "MC methods cannot be easily used, because an episode might not terminate if\n",
    "current policy caused the agent to stay in the same state.\n",
    "\n",
    "---\n",
    "section: Q-learning\n",
    "# Q-learning\n",
    "\n",
    "Q-learning was an important early breakthrough in reinforcement learning (Watkins, 1989).\n",
    "\n",
    "$$q(S_t, A_t) ‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} +  Œ≥ \\max_a q(S_{t+1}, a) -q(S_t, A_t)\\right].$$\n",
    "\n",
    "\n",
    "![w=80%,h=center](q_learning.pdf)\n",
    "\n",
    "---\n",
    "# Q-learning versus Sarsa\n",
    "\n",
    "![w=70%,h=center](cliff_walking.pdf)\n",
    "\n",
    " ~\n",
    "# Q-learning versus Sarsa\n",
    "![w=40%,h=center](cliff_walking.pdf)\n",
    "![w=45%,h=center](cliff_walking_learning.pdf)\n",
    "\n",
    "---\n",
    "section: Off-policy\n",
    "# On-policy and Off-policy Methods\n",
    "\n",
    "So far, all methods were _on-policy_. The same policy was used both for\n",
    "generating episodes and as a target of value function.\n",
    "\n",
    "\n",
    "However, while the policy for generating episodes needs to be more exploratory,\n",
    "the target policy should capture optimal behaviour.\n",
    "\n",
    "\n",
    "Generally, we can consider two policies:\n",
    "- _behaviour_ policy, usually $b$, is used to generate behaviour and can be more\n",
    "  exploratory\n",
    "\n",
    "- _target_ policy, usually $œÄ$, is the policy being learned (ideally the optimal\n",
    "  one)\n",
    "\n",
    "\n",
    "When the behaviour and target policies differ, we talk about _off-policy_\n",
    "learning.\n",
    "\n",
    "---\n",
    "# On-policy and Off-policy Methods\n",
    "\n",
    "The off-policy methods are usually more complicated and slower to converge, but\n",
    "are able to process data generated by different policy than the target one.\n",
    "\n",
    "\n",
    "The advantages are:\n",
    "- more exploratory behaviour;\n",
    "\n",
    "\n",
    "- ability to process _expert trajectories_.\n",
    "\n",
    "---\n",
    "# Off-policy Prediction\n",
    "\n",
    "Consider prediction problem for off-policy case.\n",
    "\n",
    "\n",
    "In order to use episodes from $b$ to estimate values for $œÄ$, we require that\n",
    "every action taken by $œÄ$ is also taken by $b$, i.e.,\n",
    "$$œÄ(a|s) > 0 ‚áí b(a|s) > 0.$$\n",
    "\n",
    "\n",
    "Many off-policy methods utilize _importance sampling_, a general technique for\n",
    "estimating expected values of one distribution given samples from another\n",
    "distribution.\n",
    "\n",
    "---\n",
    "# Importance Sampling\n",
    "\n",
    "Assume that $b$ and $œÄ$ are two distributions.\n",
    "\n",
    "Let $x_i$ be the samples of $b$ and $y_i$ the corresponding samples of\n",
    "$$ùîº_{x‚àºb}[f(x)].$$\n",
    "\n",
    "\n",
    "Our goal is to estimate\n",
    "$$ùîº_{x‚àºœÄ}[f(x)] = ‚àë_x œÄ(x) f(x).$$\n",
    "\n",
    "\n",
    "We can therefore compute\n",
    "$$‚àë_{x_i} \\frac{œÄ(x_i)}{b(x_i)} f(x_i)$$\n",
    "with $œÄ(x)/b(x)$ being a _relative probability_ of $x$ under the two\n",
    "distributions.\n",
    "\n",
    "---\n",
    "# Off-policy Prediction\n",
    "\n",
    "Given an initial state $S_t$ and an episode $A_t, S_{t+1}, A_{t+1}, ‚Ä¶, S_T$,\n",
    "the probability of this episode under a policy $œÄ$ is\n",
    "$$‚àè_{k=t}^{T-1} œÄ(A_k | S_k) p(S_{k+1} | S_k, A_k).$$\n",
    "\n",
    "\n",
    "Therefore, the relative probability of a trajectory under the target and\n",
    "behaviour policies is\n",
    "$$œÅ_t ‚âù \\frac{‚àè_{k=t}^{T-1} œÄ(A_k | S_k) p(S_{k+1} | S_k, A_k)}{‚àè_{k=t}^{T-1} b(A_k | S_k) p(S_{k+1} | S_k, A_k)}\n",
    "      = ‚àè_{k=t}^{T-1} \\frac{œÄ(A_k | S_k)}{b(A_k | S_k)}.$$\n",
    "\n",
    "\n",
    "Therefore, if $G_t$ is a return of episode generated according to $b$, we can\n",
    "estimate\n",
    "$$v_œÄ(S_t) = ùîº_b[œÅ_t G_t].$$\n",
    "\n",
    "---\n",
    "# Off-policy Monte Carlo Prediction\n",
    "\n",
    "Let $ùì£(s)$ be a set of times when we visited state $s$. Given episodes sampled\n",
    "according to $b$, we can estimate\n",
    "$$v_œÄ(s) = \\frac{‚àë_{t‚ààùì£(s)} œÅ_t G_t}{|ùì£(s)|}.$$\n",
    "\n",
    "\n",
    "Such simple average is called _ordinary importance sampling_. It is unbiased, but\n",
    "can have very high variance.\n",
    "\n",
    "\n",
    "An alternative is _weighted importance sampling_, where we compute weighted\n",
    "average as\n",
    "$$v_œÄ(s) = \\frac{‚àë_{t‚ààùì£(s)} œÅ_t G_t}{‚àë_{t‚ààùì£(s)} œÅ_t}.$$\n",
    "\n",
    "\n",
    "Weighted importance sampling is biased (with bias asymptotically converging to\n",
    "zero), but usually has smaller variance.\n",
    "\n",
    "---\n",
    "# Off-policy Monte Carlo Prediction\n",
    "\n",
    "![w=80%,h=center](importance_sampling.pdf)\n",
    "\n",
    "Comparison of ordinary and weighted importance sampling on Blackjack. Given\n",
    "a state with sum of player's cards 13 and a usable ace, we estimate target\n",
    "policy of sticking only with a sum of 20 and 21, using uniform behaviour policy.\n",
    "\n",
    "---\n",
    "# Off-policy Monte Carlo Prediction\n",
    "\n",
    "We can compute weighted importance sampling similarly to the incremental\n",
    "implementation of Monte Carlo averaging.\n",
    "\n",
    "![w=75%,h=center](off_policy_mc_prediction.pdf)\n",
    "\n",
    "---\n",
    "# Off-policy Monte Carlo\n",
    "\n",
    "![w=80%,h=center](off_policy_mc.pdf)\n",
    "\n",
    "---\n",
    "section: Expected Sarsa\n",
    "# Expected Sarsa\n",
    "\n",
    "The action $A_{t+1}$ is a source of variance, moving only _in expectation_.\n",
    "\n",
    "\n",
    "We could improve the algorithm by considering all actions proportionally to their\n",
    "policy probability, obtaining Expected Sarsa algorithm:\n",
    "$$\\begin{aligned}\n",
    "  q(S_t, A_t) &‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ ùîº_œÄ q(S_{t+1}, a) - q(S_t, A_t)\\right]\\\\\n",
    "              &‚Üê q(S_t, A_t) + Œ±\\left[R_{t+1} + Œ≥ ‚àë\\nolimits_a œÄ(a|S_{t+1}) q(S_{t+1}, a) - q(S_t, A_t)\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "Compared to Sarsa, the expectation removes a source of variance and therefore\n",
    "usually performs better. However, the complexity of the algorithm increases and\n",
    "becomes dependent on number of actions $|ùìê|$.\n",
    "\n",
    "---\n",
    "# Expected Sarsa as Off-policy Algorithm\n",
    "\n",
    "Note that Expected Sarsa is also an off-policy algorithm, allowing the behaviour\n",
    "policy $b$ and target policy $œÄ$ to differ.\n",
    "\n",
    "\n",
    "Especially, if $œÄ$ is a greedy policy with respect to current value function,\n",
    "Expected Sarsa simplifies to Q-learning.\n",
    "\n",
    "---\n",
    "# Expected Sarsa Example\n",
    "\n",
    "![w=25%](cliff_walking.pdf)![w=90%,mw=75%,h=center](expected_sarsa.pdf)\n",
    "\n",
    "Asymptotic performance is averaged over 100k episodes, interim performance\n",
    "over the first 100."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
