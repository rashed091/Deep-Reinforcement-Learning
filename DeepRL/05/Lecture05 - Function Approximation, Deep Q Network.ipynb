{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title: NPFL122, Lecture 5\n",
    "class: title, langtech, cc-by-nc-sa\n",
    "# Function Approximation,<br>Deep Q Network\n",
    "\n",
    "## Milan Straka\n",
    "\n",
    "### November 12, 2018\n",
    "\n",
    "---\n",
    "section: Refresh\n",
    "# $n$-step Methods\n",
    "\n",
    "![w=40%,f=right](../04/nstep_td.pdf)\n",
    "\n",
    "Full return is\n",
    "$$G_t = ‚àë_{k=t}^‚àû R_{k+1},$$\n",
    "one-step return is\n",
    "$$G_{t:t+1} = R_{t+1} + Œ≥ V(S_{t+1}).$$\n",
    "\n",
    "We can generalize both into $n$-step returns:\n",
    "$$G_{t:t+n} ‚âù \\left(‚àë_{k=t}^{t+n-1} Œ≥^{k-t} R_{k+1}\\right) + Œ≥^n V(S_{t+n}).$$\n",
    "with $G_{t:t+n} ‚âù G_t$ if $t+n ‚â• T$.\n",
    "\n",
    "---\n",
    "# $n$-step Sarsa\n",
    "\n",
    "![w=30%,f=right](../04/nstep_td.pdf)\n",
    "\n",
    "Defining the $n$-step return to utilize action-value function as\n",
    "$$G_{t:t+n} ‚âù \\left(‚àë_{k=t}^{t+n-1} Œ≥^{k-t} R_{k+1}\\right) + Œ≥^n Q(S_{t+n}, A_{t+n})$$\n",
    "with $G_{t:t+n} ‚âù G_t$ if $t+n ‚â• T$, we get the following straightforward\n",
    "update rule:\n",
    "$$Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±\\left[G_{t:t+n} - Q(S_t, A_t)\\right].$$\n",
    "\n",
    "![w=55%,h=center](../04/nstep_sarsa_example.pdf)\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Sarsa\n",
    "\n",
    "Recall the relative probability of a trajectory under the target and behaviour policies,\n",
    "which we now generalize as\n",
    "$$œÅ_{t:t+n} ‚âù ‚àè_{k=t}^{\\min(t+n, T-1)} \\frac{œÄ(A_k | S_k)}{b(A_k | S_k)}.$$\n",
    "\n",
    "\n",
    "Then a simple off-policy $n$-step TD can be computed as\n",
    "$$V(S_t) ‚Üê V(S_t) + Œ±œÅ_{t:t+n-1}\\left[G_{t:t+n} - V(S_t)\\right].$$\n",
    "\n",
    "\n",
    "Similarly, $n$-step Sarsa becomes\n",
    "$$Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±œÅ_{\\boldsymbol{t+1}:\\boldsymbol{t+n}}\\left[G_{t:t+n} - Q(S_t, A_t)\\right].$$\n",
    "\n",
    "---\n",
    "# Off-policy $n$-step Without Importance Sampling\n",
    "\n",
    "![w=10%,f=right](../04/tree_backup_example.pdf)\n",
    "\n",
    "We now derive the $n$-step reward, starting from one-step:\n",
    "$$G_{t:t+1} ‚âù R_{t+1} + ‚àë\\nolimits_a œÄ(a|S_{t+1}) Q(S_{t+1}, a).$$\n",
    "\n",
    "\n",
    "For two-step, we get:\n",
    "$$G_{t:t+2} ‚âù R_{t+1} + Œ≥‚àë\\nolimits_{a‚â†A_{t+1}} œÄ(a|S_{t+1}) Q(S_{t+1}, a) + Œ≥œÄ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$\n",
    "\n",
    "\n",
    "Therefore, we can generalize to:\n",
    "$$G_{t:t+n} ‚âù R_{t+1} + Œ≥‚àë\\nolimits_{a‚â†A_{t+1}} œÄ(a|S_{t+1}) Q(S_{t+1}, a) + Œ≥œÄ(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$\n",
    "\n",
    "---\n",
    "# Function Approximation\n",
    "\n",
    "We will approximate value function $v$ and/or state-value function $q$, choosing\n",
    "from a family of functions parametrized by a weight vector $‚Üíw‚àà‚Ñù^d$.\n",
    "\n",
    "We denote the approximations as\n",
    "$$\\begin{gathered}\n",
    "  vÃÇ(s, ‚Üíw),\\\\\n",
    "  qÃÇ(s, a, ‚Üíw).\n",
    "\\end{gathered}$$\n",
    "\n",
    "\n",
    "We utilize the _Mean Squared Value Error_ objective, denoted $\\overline{VE}$:\n",
    "$$\\overline{VE}(‚Üíw) ‚âù ‚àë_{s‚ààùì¢} Œº(s) \\left[v_œÄ(s) - vÃÇ(s, ‚Üíw)\\right]^2,$$\n",
    "where the state distribution $Œº(s)$ is usually on-policy distribution.\n",
    "\n",
    "---\n",
    "# Gradient and Semi-Gradient Methods\n",
    "\n",
    "The functional approximation (i.e., the weight vector $‚Üíw$) is usually optimized\n",
    "using gradient methods, for example as\n",
    "$$\\begin{aligned}\n",
    "  ‚Üíw_{t+1} &‚Üê ‚Üíw_t - \\frac{1}{2} Œ± ‚àá \\left[v_œÄ(S_t) - vÃÇ(S_t, ‚Üíw_t)\\right]^2\\\\\n",
    "           &‚Üê ‚Üíw_t - Œ±\\left[v_œÄ(S_t) - vÃÇ(S_t, ‚Üíw_t)\\right] ‚àá vÃÇ(S_t, ‚Üíw_t).\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "As usual, the $v_œÄ(S_t)$ is estimated by a suitable sample. For example in Monte\n",
    "Carlo methods, we use episodic return $G_t$, and in temporal difference methods,\n",
    "we employ bootstrapping and use $R_{t+1} + Œ≥vÃÇ(S_{t+1}, ‚Üíw).$\n",
    "\n",
    "---\n",
    "# Monte Carlo Gradient Policy Evaluation\n",
    "![w=100%,v=middle](../04/grad_mc_estimation.pdf)\n",
    "\n",
    "---\n",
    "# Linear Methods\n",
    "\n",
    "A simple special case of function approximation are linear methods, where\n",
    "$$vÃÇ(‚Üíx(s), ‚Üíw) ‚âù ‚Üíx(s)^T ‚Üíw = ‚àëx(s)_i w_i.$$\n",
    "\n",
    "The $‚Üíx(s)$ is a representation of state $s$, which is a vector of the same size\n",
    "as $‚Üíw$. It is sometimes called a _feature vector_.\n",
    "\n",
    "The SGD update rule then becomes\n",
    "$$‚Üíw_{t+1} ‚Üê ‚Üíw_t - Œ±\\left[v_œÄ(S_t) - vÃÇ(‚Üíx(S_t), ‚Üíw_t)\\right] ‚Üíx(S_t).$$\n",
    "\n",
    "---\n",
    "# Feature Construction for Linear Methods\n",
    "\n",
    "Many methods developed in the past:\n",
    "\n",
    "- state aggregation,\n",
    "\n",
    "- polynomials\n",
    "\n",
    "- Fourier basis\n",
    "\n",
    "- tile coding\n",
    "\n",
    "- radial basis functions\n",
    "\n",
    "But of course, nowadays we use deep neural networks which construct a suitable\n",
    "feature vector automatically as a latent variable (the last hidden layer).\n",
    "\n",
    "---\n",
    "# State Aggregation\n",
    "\n",
    "Simple way of generating a feature vector is _state aggregation_, where several\n",
    "neighboring states are grouped together.\n",
    "\n",
    "For example, consider a 1000-state random walk, where transitions go uniformly\n",
    "randomly to any of 100 neighboring states on the left or on the right. Using\n",
    "state aggregation, we can partition the 1000 states into 10 groups of 100\n",
    "states. Monte Carlo policy evaluation then computes the following:\n",
    "\n",
    "![w=60%,h=center](../04/grad_mc_estimation_example.pdf)\n",
    "\n",
    "---\n",
    "section: Tile Coding\n",
    "# Tile Coding\n",
    "\n",
    "![w=100%,mh=90%,v=middle](../04/tile_coding.pdf)\n",
    "\n",
    "If $t$ overlapping tiles are used, the learning rate is usually normalized as $Œ±/t$.\n",
    "\n",
    "---\n",
    "# Tile Coding\n",
    "\n",
    "For example, on the 1000-state random walk example, the performance of tile\n",
    "coding surpasses state aggregation:\n",
    "\n",
    "![w=60%,h=center](../04/tile_coding_performance.pdf)\n",
    "\n",
    "---\n",
    "# Asymmetrical Tile Coding\n",
    "\n",
    "In higher dimensions, the tiles should have asymmetrical offsets, with\n",
    "a sequence of $(1, 3, 5, ‚Ä¶, 2d-1)$ being a good choice.\n",
    "\n",
    "![w=50%,h=center](../04/tile_coding_asymmetrical.pdf)\n",
    "\n",
    "---\n",
    "section: Semi-Gradient TD\n",
    "# Temporal Difference Semi-Gradient Policy Evaluation\n",
    "\n",
    "In TD methods, we again use bootstrapping to estimate\n",
    "$v_œÄ(S_t)$ as $R_{t+1} + Œ≥vÃÇ(S_{t+1}, ‚Üíw).$\n",
    "\n",
    "\n",
    "![w=70%,h=center](grad_td_estimation.pdf)\n",
    "\n",
    "\n",
    "Note that such algorithm is called _semi-gradient_, because it does not\n",
    "backpropagate through $vÃÇ(S', ‚Üíw)$.\n",
    "\n",
    "---\n",
    "# Temporal Difference Semi-Gradient Policy Evaluation\n",
    "\n",
    "An important fact is that linear semi-gradient TD methods do not converge to\n",
    "$\\overline{VE}$. Instead, they converge to a different _TD fixed point_\n",
    "$‚Üíw_\\mathrm{TD}$.\n",
    "\n",
    "\n",
    "It can be proven that\n",
    "$$\\overline{VE}(‚Üíw_\\mathrm{TD}) ‚â§ \\frac{1}{1-Œ≥} \\min_‚Üíw \\overline{VE}(‚Üíw).$$\n",
    "\n",
    "\n",
    "However, when $Œ≥$ is close to one, the multiplication factor in the above bound\n",
    "is quite large.\n",
    "\n",
    "---\n",
    "# Temporal Difference Semi-Gradient Policy Evaluation\n",
    "\n",
    "As before, we can utilize $n$-step TD methods.\n",
    "\n",
    "![w=60%,h=center](grad_td_nstep_estimation.pdf)\n",
    "\n",
    "---\n",
    "# Temporal Difference Semi-Gradient Policy Evaluation\n",
    "\n",
    "![w=100%,v=middle](grad_td_estimation_example.pdf)\n",
    "\n",
    "---\n",
    "# Sarsa with Function Approximation\n",
    "\n",
    "Until now, we talked only about policy evaluation. Naturally, we can extend it\n",
    "to a full Sarsa algorithm:\n",
    "\n",
    "![w=80%,h=center](grad_sarsa.pdf)\n",
    "\n",
    "---\n",
    "# Sarsa with Function Approximation\n",
    "\n",
    "Additionally, we can incorporate $n$-step returns:\n",
    "\n",
    "![w=55%,h=center](grad_sarsa_nstep.pdf)\n",
    "\n",
    "---\n",
    "# Mountain Car Example\n",
    "\n",
    "![w=65%,h=center](mountain_car.png)\n",
    "\n",
    "The performances are for semi-gradient Sarsa($Œª$) algorithm (which we did not\n",
    "talked about yet) with tile coding of 8 overlapping tiles covering position and\n",
    "velocity, with offsets of $(1, 3)$.\n",
    "\n",
    "---\n",
    "# Mountain Car Example\n",
    "\n",
    "![w=50%,h=center](mountain_car_performance_1and8_step.pdf)\n",
    "![w=50%,h=center](mountain_car_performance_nstep.pdf)\n",
    "\n",
    "---\n",
    "section: Off-policy Divergence\n",
    "# Off-policy Divergence With Function Approximation\n",
    "\n",
    "Consider a deterministic transition between two states whose values are computed\n",
    "using the same weight:\n",
    "\n",
    "![w=20%,h=center](off_policy_divergence_idea.pdf)\n",
    "\n",
    "\n",
    "- If initially $w=10$, TD error will be also 10 (or nearly 10 if $Œ≥<1$).\n",
    "\n",
    "- If for example $Œ±=0.1$, $w$ will be increased to 1 (by 10%).\n",
    "\n",
    "- This process can continue indefinitely.\n",
    "\n",
    "\n",
    "However, the problem arises only in off-policy setting, where we do not decrease\n",
    "value of the second state from further observation.\n",
    "\n",
    "---\n",
    "# Off-policy Divergence With Function Approximation\n",
    "\n",
    "The previous idea can be realized for instance by the following example.\n",
    "\n",
    "![w=80%,h=center](off_policy_divergence_example.pdf)\n",
    "\n",
    "---\n",
    "# Off-policy Divergence With Function Approximation\n",
    "\n",
    "![w=35%,v=middle](off_policy_divergence_example.pdf)![w=65%,v=middle](off_policy_divergence_results.pdf)\n",
    "\n",
    "---\n",
    "section: DQN\n",
    "# Deep Q Networks\n",
    "\n",
    "Volodymyr Mnih et al.: _Playing Atari with Deep Reinforcement Learning_ (Dec 2013 on arXiv).\n",
    "\n",
    "\n",
    "In 2015 accepted in Nature, as _Human-level control through deep reinforcement learning_.\n",
    "\n",
    "\n",
    "Off-policy Q-learning algorithm with a convolutional neural network function\n",
    "approximation of action-value function.\n",
    "\n",
    "\n",
    "Training can be extremely brittle (and can even diverge as shown earlier).\n",
    "\n",
    "---\n",
    "# Deep Q Network\n",
    "\n",
    "![w=85%,h=center](dqn_architecture.pdf)\n",
    "\n",
    "---\n",
    "# Deep Q Network\n",
    "\n",
    "![w=40%,h=center](dqn_results.pdf)\n",
    "\n",
    "---\n",
    "# Deep Q Network\n",
    "\n",
    "![w=80%,h=center](dqn_visualization_breakout.pdf)\n",
    "\n",
    "---\n",
    "# Deep Q Network\n",
    "\n",
    "![w=100%,v=middle](dqn_visualization_pong.pdf)\n",
    "\n",
    "---\n",
    "# Deep Q Networks\n",
    "\n",
    "- Preprocessing: $210√ó160$ 128-color images are converted to grayscale and\n",
    "  then resized to $84√ó84$.\n",
    "\n",
    "- Frame skipping technique is used, i.e., only every $4^\\textrm{th}$ frame\n",
    "  (out of 60 per second) is considered, and the selected action is repeated on\n",
    "  the other frames.\n",
    "\n",
    "- Input to the network are last $4$ frames (considering only the frames kept by\n",
    "  frame skipping), i.e., an image with $4$ channels.\n",
    "\n",
    "- The network is fairly standard, performing\n",
    "  - 32 filters of size $8√ó8$ with stride 4 and ReLU,\n",
    "  - 64 filters of size $4√ó4$ with stride 2 and ReLU,\n",
    "  - 64 filters of size $3√ó3$ with stride 1 and ReLU,\n",
    "  - fully connected layer with 512 units and ReLU,\n",
    "  - output layer with 18 output units (one for each action)\n",
    "\n",
    "---\n",
    "# Deep Q Networks\n",
    "\n",
    "- Network is trained with RMSProp to minimize the following loss:\n",
    "  $$ùìõ ‚âù ùîº_{(s, a, r, s')‚àº\\mathit{data}}\\left[(r + Œ≥ \\max_{a'} Q(s', a'; Œ∏ÃÑ) - Q(s, a; Œ∏))^2\\right].$$\n",
    "\n",
    "- An $Œµ$-greedy behavior policy is utilized.\n",
    "\n",
    "\n",
    "Important improvements:\n",
    "\n",
    "- experience replay: the generated episodes are stored in a buffer as $(s, a, r,\n",
    "  s')$ quadruples, and for training a transition is sampled uniformly;\n",
    "\n",
    "- separate target network $Œ∏ÃÑ$: to prevent instabilities, a separate target\n",
    "  network is used to estimate state-value function. The weights are not trained,\n",
    "  but copied from the trained network once in a while;\n",
    "\n",
    "- reward clipping of $(r + Œ≥ \\max_{a'} Q(s', a'; Œ∏ÃÑ) - Q(s, a; Œ∏))$ to $[-1, 1]$.\n",
    "\n",
    "---\n",
    "class: tablefull\n",
    "# Deep Q Networks Hyperparameters\n",
    "\n",
    "| Hyperparameter | Value |\n",
    "|----------------|-------|\n",
    "| minibatch size | 32 |\n",
    "| replay buffer size | 1M |\n",
    "| target network update frequency | 10k |\n",
    "| discount factor | 0.99 |\n",
    "| training frames | 50M |\n",
    "| RMSProp learning rate and momentum | 0.00025, 0.95 |\n",
    "| initial $Œµ$, final $Œµ$ and frame of final $Œµ$ | 1.0, 0.1, 1M |\n",
    "| replay start size | 50k |\n",
    "| no-op max | 30 |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
