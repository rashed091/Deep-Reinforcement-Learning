{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "title: NPFL122, Lecture 9\n",
    "class: title, langtech, cc-by-nc-sa\n",
    "# Deterministic Policy Gradient, Advanced RL Algorithms\n",
    "\n",
    "## Milan Straka\n",
    "\n",
    "### December 10, 2018\n",
    "\n",
    "---\n",
    "section: Refresh\n",
    "# REINFORCE with Baseline\n",
    "\n",
    "The returns can be arbitrary – better-than-average and worse-than-average\n",
    "returns cannot be recognized from the absolute value of the return.\n",
    "\n",
    "Hopefully, we can generalize the policy gradient theorem using a baseline $b(s)$\n",
    "to\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} \\big(q_π(s, a) - b(s)\\big) ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "A good choice for $b(s)$ is $v_π(s)$, which can be shown to minimize variance of\n",
    "the estimator. Such baseline reminds centering of returns, given that\n",
    "$v_π(s) = 𝔼_{a ∼ π} q_π(s, a)$. Then, better-than-average returns are positive\n",
    "and worse-than-average returns are negative.\n",
    "\n",
    "The resulting value is also called an _advantage function_\n",
    "$a_π(s, a) ≝ q_π(s, a) - v_π(s)$.\n",
    "\n",
    "Of course, the $v_π(s)$ baseline can be only approximated. If neural networks\n",
    "are used to estimate $π(a|s; →θ)$, then some part of the network is usually\n",
    "shared between the policy and value function estimation, which is trained using\n",
    "mean square error of the predicted and observed return.\n",
    "\n",
    "---\n",
    "# Parallel Advantage Actor Critic\n",
    "\n",
    "An alternative to independent workers is to train in a synchronous and\n",
    "centralized way by having the workes to only generate episodes. Such approach\n",
    "was described in May 2017 by Celemente et al., who named their agent\n",
    "_parallel advantage actor-critic_ (PAAC).\n",
    "\n",
    "![w=70%,h=center](../08/paac_framework.pdf)\n",
    "\n",
    "---\n",
    "# Continuous Action Space\n",
    "\n",
    "Until now, the actions were discreet. However, many environments naturally\n",
    "accept actions from continuous space. We now consider actions which come\n",
    "from range $[a, b]$ for $a, b ∈ ℝ$, or more generally from a Cartesian product\n",
    "of several such ranges:\n",
    "$$∏_i [a_i, b_i].$$\n",
    "\n",
    "![w=40%,f=right](../08/normal_distribution.pdf)\n",
    "A simple way how to parametrize the action distribution is to choose them from\n",
    "the normal distribution.\n",
    "\n",
    "Given mean $μ$ and variance $σ^2$, probability density function of $𝓝(μ, σ^2)$\n",
    "is\n",
    "$$p(x) ≝ \\frac{1}{\\sqrt{2 π σ^2}} e^{\\large-\\frac{(x - μ)^2}{2σ^2}}.$$\n",
    "\n",
    "---\n",
    "# Continuous Action Space in Gradient Methods\n",
    "\n",
    "Utilizing continuous action spaces in gradient-based methods is straightforward.\n",
    "Instead of the $\\softmax$ distribution we suitably parametrize the action value,\n",
    "usually using the normal distribution. Considering only one real-valued action,\n",
    "we therefore have\n",
    "$$π(a | s; →θ) ≝ P\\Big(a ∼ 𝓝\\big(μ(s; →θ), σ(s; →θ)^2\\big)\\Big),$$\n",
    "where $μ(s; →θ)$ and $σ(s; →θ)$ are function approximation of mean and standard\n",
    "deviation of the action distribution.\n",
    "\n",
    "The mean and standard deviation are usually computed from the shared\n",
    "representation, with\n",
    "- the mean being computed as a regular regression (i.e., one output neuron\n",
    "  without activation);\n",
    "- the standard variance (which must be positive) being computed again as\n",
    "  a regression, followed most commonly by either $\\exp$ or\n",
    "  $\\operatorname{softplus}$, where $\\operatorname{softplus}(x) ≝ \\log(1 + e^x)$.\n",
    "\n",
    "---\n",
    "# Continuous Action Space in Gradient Methods\n",
    "\n",
    "During training, we compute $μ(s; →θ)$ and $σ(s; →θ)$ and then sample the action\n",
    "value (clipping it to $[a, b]$ if required). To compute the loss, we utilize\n",
    "the probability density function of the normal distribution (and usually also\n",
    "add the entropy penalty).\n",
    "\n",
    "```python\n",
    "  mu = tf.layers.dense(hidden_layer, 1)[:, 0]\n",
    "  sd = tf.layers.dense(hidden_layer, 1)[:, 0]\n",
    "  sd = tf.exp(log_sd)   # or sd = tf.nn.softplus(sd)\n",
    "\n",
    "  normal_dist = tf.distributions.Normal(mu, sd)\n",
    "\n",
    "  # Loss computed as - log π(a|s) - entropy_regularization\n",
    "  loss = - normal_dist.log_prob(self.actions) * self.returns \\\n",
    "         - args.entropy_regularization * normal_dist.entropy()\n",
    "```\n",
    "\n",
    "---\n",
    "section: DPG\n",
    "# Deterministic Policy Gradient Theorem\n",
    "\n",
    "Combining continuous actions and Deep Q Networks is not straightforward.\n",
    "In order to do so, we need a different variant of the policy gradient theorem.\n",
    "\n",
    "\n",
    "Recall that in policy gradient theorem,\n",
    "$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$\n",
    "\n",
    "\n",
    "## Deterministic Policy Gradient Theorem\n",
    "Assume that the policy $π(s; →θ)$ is deterministic and computes\n",
    "an action $a∈ℝ$. Then under several assumptions about continuousness, the\n",
    "following holds:\n",
    "$$∇_→θ J(→θ) ∝ 𝔼_{s∼μ(s)} \\Big[∇_→θ π(s; →θ) ∇_a q_π(s, a)\\big|_{a=π(s;→θ)}\\Big].$$\n",
    "\n",
    "The theorem was first proven in the paper Deterministic Policy Gradient Algorithms\n",
    "by David Silver et al.\n",
    "\n",
    "---\n",
    "# Deterministic Policy Gradient Theorem – Proof\n",
    "\n",
    "The proof is very similar to the original (stochastic) policy gradient theorem.\n",
    "We assume that $p(s' | s, a), ∇_a p(s' | s, a), r(s, a), ∇_a r(s, a), π(s; →θ), ∇_→θ π(s; →θ)$\n",
    "are continuous in all params.\n",
    "\n",
    "\n",
    "$\\displaystyle ∇_→θ v_π(s) = ∇_→θ q_π(s, π(s; →θ))$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇_→θ v_π(s)} = ∇_→θ\\Big(r\\big(s, π(s; →θ)\\big) + γ ∫_{s'} p\\big(s' | s, π(s; →θ)\\big) v_π(s') \\d s'\\Big)$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a r(s, a) \\big|_{a=π(s; →θ)} + γ ∇_→θ ∫_{s'} p\\big(s' | s, π(s; →θ)\\big) v_π(s') \\d s'$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a \\Big( r(s, a) \\big|_{a=π(s; →θ)} + γ ∫_{s'} p\\big(s' | s, a)\\big) v_π(s') \\d s' \\Big) \\\\\n",
    "                    \\qquad\\qquad\\qquad + γ ∫_{s'} p\\big(s' | s, π(s; →θ)\\big) ∇_→θ v_π(s') \\d s'$\n",
    "\n",
    "\n",
    "$\\displaystyle \\phantom{∇_→θ v_π(s)} = ∇_→θ π(s; →θ) ∇_a q_π(s, a)\\big|_{a=π(s; →θ)} + γ ∫_{s'} p\\big(s' | s, π(s; →θ)\\big) ∇_→θ v_π(s') \\d s'$\n",
    "\n",
    "\n",
    "Similarly to the gradient theorem, we finish the proof by continually expanding $∇_→θ v_π(s')$.\n",
    "\n",
    "---\n",
    "section: DDPG\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "Note that the formulation of deterministic policy gradient theorem allows an\n",
    "off-policy algorithm, because the loss functions no longer depends on actions\n",
    "(similarly to how expected Sarsa is also an off-policy algorithm).\n",
    "\n",
    "\n",
    "We therefore train function approximation for both $π(s; →θ)$ and $q(s, a; →θ)$,\n",
    "training $q(s, a; →θ)$ using a deterministic variant of the Bellman equation:\n",
    "$$q(S_t, A_t; →θ) = 𝔼_{R_{t+1}, S_{t+1}} \\big[R_{t+1} + γ q(S_{t+1}, π(S_{t+1}; →θ))\\big]$$\n",
    "and $π(s; →θ)$ according to the deterministic policy gradient theorem.\n",
    "\n",
    "\n",
    "The algorithm was first described in the paper Continuous Control with Deep Reinforcement Learning\n",
    "by Timothy P. Lillicrap et al. (2015).\n",
    "\n",
    "The authors utilize a replay buffer, a target network (updated by exponential\n",
    "moving average with $τ=0.001$), batch normalization for CNNs, and perform\n",
    "exploration by adding a normal-distributed noise to predicted actions.\n",
    "Training is performed by Adam with learning rates of 1e-4 and 1e-3 for the\n",
    "policy and critic network, respectively.\n",
    "\n",
    "---\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "![w=65%,h=center](ddpg.pdf)\n",
    "\n",
    "---\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "![w=100%](ddpg_ablation.pdf)\n",
    "\n",
    "---\n",
    "# Deep Deterministic Policy Gradients\n",
    "\n",
    "Results using low-dimensional (_lowd_) version of the environment, pixel representation\n",
    "(_pix_) and DPG reference (_cntrl_).\n",
    "\n",
    "![w=57%,h=center](ddpg_results.pdf)\n",
    "\n",
    "---\n",
    "section: NPG\n",
    "# Natural Policy Gradient\n",
    "\n",
    "The following approach has been introduced by Kakade (2002).\n",
    "\n",
    "\n",
    "Using policy gradient theorem, we are able to compute $∇ v_π$. Normally, we\n",
    "update the parameters by using directly this gradient. This choice is justified\n",
    "by the fact that a vector $→d$ which maximizes $v_π(s; →θ + →d)$ under\n",
    "the constraint that $|→d|^2$ is bounded by a small constant is exactly\n",
    "the gradient $∇ v_π$.\n",
    "\n",
    "\n",
    "Normally, the length $|→d|^2$ is computed using Euclidean metric. But in general,\n",
    "any metric could be used. Representing a metric using a positive-definite matrix\n",
    "$⇉G$ (identity matrix for Euclidean metric), we can compute the distance as\n",
    "$|→d|^2 = ∑_{ij} G_{ij} d_i d_j = →d^T ⇉G →d$. The steepest ascent direction is\n",
    "then given by $⇉G^{-1} ∇ v_π$.\n",
    "\n",
    "\n",
    "Note that when $⇉G$ is the Hessian $⇉H v_π$, the above process is exactly\n",
    "Newton's method.\n",
    "\n",
    "---\n",
    "# Natural Policy Gradient\n",
    "\n",
    "![w=100%,v=middle](npg.pdf)\n",
    "\n",
    "---\n",
    "# Natural Policy Gradient\n",
    "\n",
    "A suitable choice for the metric is _Fisher information matrix_ defined as\n",
    "$$F_s(→θ) ≝ 𝔼_{π(a | s; →θ)} \\left[\\frac{∂ \\log π(a | s; →θ)}{∂ →θ_i} \\frac{∂ \\log π(a | s; →θ)}{∂ →θ_j} \\right]\n",
    "\\color{gray} = 𝔼[∇ π(a | s; →θ)] 𝔼[∇ π(a | s; →θ)]^T.$$\n",
    "\n",
    "\n",
    "It can be shown that the Fisher information metric is the only Riemannian metric\n",
    "(up to rescaling) invariant to change of parameters under sufficient statistic.\n",
    "\n",
    "\n",
    "Recall Kullback-Leibler distance (or relative entropy) defined as\n",
    "$$D_\\textrm{KL}(→p || →q) ≝ ∑_i p_i \\log \\frac{p_i}{q_i} \\color{gray} = H(p, q) - H(p).$$\n",
    "\n",
    "\n",
    "The Fisher information matrix is also a Hessian of the\n",
    "$D_\\textrm{KL}(π(a | s; →θ) || π(a | s; →θ')$:\n",
    "$$F_s(→θ) = \\frac{∂^2}{∂θ_i' ∂θ_j'} D_\\textrm{KL}(π(a | s; →θ) || π(a | s; →θ')\\Big|_{→θ' = →θ}.$$\n",
    "\n",
    "---\n",
    "# Natural Policy Gradient\n",
    "\n",
    "Using the metric\n",
    "$$F(→θ) = 𝔼_{s ∼ μ_→θ} F_s(→θ)$$\n",
    "we want to update the parameters using $→d_F ≝ F(→θ)^{-1} ∇ v_π$.\n",
    "\n",
    "\n",
    "An interesting property of using the $→d_F$ to update the parameters is that\n",
    "- updating $→θ$ using $∇ v_π$ will choose an arbitrary _better_ action in state\n",
    "  $s$;\n",
    "\n",
    "- updating $→θ$ using $F(→θ)^{-1} ∇ v_π$ chooses the _best_ action (maximizing\n",
    "  expected return), similarly to tabular greedy policy improvement.\n",
    "\n",
    "\n",
    "However, computing $→d_F$ in a straightforward way is too costly.\n",
    "\n",
    "---\n",
    "# Truncated Natural Policy Gradient\n",
    "\n",
    "Duan et al. (2016) in paper _Benchmarking Deep Reinforcement Learning for\n",
    "Continuous Control_ propose a modification to the NPG to efficiently compute\n",
    "$→d_F$.\n",
    "\n",
    "\n",
    "Following Schulman et al. (2015), they suggest to use _conjugate gradient\n",
    "algorithm_, which can solve a system of linear equations $⇉A→x = →b$\n",
    "in an iterative manner, by using $⇉A$ only to compute products $⇉A→v$ for\n",
    "a suitable $→v$.\n",
    "\n",
    "\n",
    "Therefore, $→d_F$ is found as a solution of\n",
    "$$F(→θ)→d_F = ∇ v_π$$\n",
    "and using only 10 iterations of the algorithm seem to suffice according to the\n",
    "experiments.\n",
    "\n",
    "\n",
    "Furthermore, Duan et al. suggest to use a specific learning rate suggested by\n",
    "Peters et al (2008) of\n",
    "$$\\frac{α}{\\sqrt{(∇ v_π)^T F(→θ)^{-1} ∇ v_π}}.$$\n",
    "\n",
    "---\n",
    "section: TRPO\n",
    "# Trust Region Policy Optimization\n",
    "\n",
    "Schulman et al. in 2015 wrote an influential paper introducing TRPO as an\n",
    "improved variant of NPG.\n",
    "\n",
    "\n",
    "Considering two policies $π, π̃$, we can write\n",
    "$$v_π̃ = v_π + 𝔼_{s ∼ μ(π̃)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s),$$\n",
    "where $a_π(a | s)$ is the advantage function $q_π(a | s) - v_π(s)$ and\n",
    "$μ(π̃)$ is the on-policy distribution of the policy $π̃$.\n",
    "\n",
    "\n",
    "Analogously to policy improvement, we see that if $a_π(a | s) ≥0$, policy\n",
    "$π̃$ performance increases (or stays the same if the advantages are zero\n",
    "everywhere).\n",
    "\n",
    "\n",
    "However, sampling states $s ∼ μ(π̃)$ is costly. Therefore, we instead\n",
    "consider\n",
    "$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s).$$\n",
    "\n",
    "---\n",
    "# Trust Region Policy Optimization\n",
    "$$L_π(π̃) = v_π + 𝔼_{s ∼ μ(π)} 𝔼_{a ∼ π̃(a | s)} a_π(a | s)$$\n",
    "\n",
    "It can be shown that for parametrized $π(a | s; →θ)$ the $L_π(π̃)$ matches\n",
    "$v_{π̃}$ to the first order.\n",
    "\n",
    "\n",
    "Schulman et al. additionally proves that if we denote\n",
    "$α = D_\\textrm{KL}^\\textrm{max}(π_\\textrm{old} || π_\\textrm{new})\n",
    "   = \\max_s D_\\textrm{KL}\\big(π_\\textrm{old}(⋅|s) || π_\\textrm{new}(⋅|s)\\big)$, then\n",
    "$$v_{π_\\textrm{new}} ≥ L_{π_\\textrm{old}}(π_\\textrm{new}) - \\frac{4εγ}{(1-γ)^2}α\\textrm{where}ε = \\max_{s, a} |a_π(s, a)|.$$\n",
    "\n",
    "\n",
    "Therefore, TRPO minimizes $L_{π_{→θ_0}}(π_→θ)$ subject to\n",
    "$D_\\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) < δ$, where\n",
    "- $D_\\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) = 𝔼_{s ∼ μ(π_{→θ_0})} [D_\\textrm{KL}\\big(π_\\textrm{old}(⋅|s) || π_\\textrm{new}(⋅|s)\\big)]$\n",
    "  is used instead of $D_\\textrm{KL}^\\textrm{max}$ for performance reasons;\n",
    "\n",
    "- $δ$ is a constant found empirically, as the one implied by the above equation\n",
    "  is too small;\n",
    "\n",
    "- importance sampling is used to account for sampling actions from $π$.\n",
    "\n",
    "---\n",
    "# Trust Region Policy Optimization\n",
    "\n",
    "$$\\textrm{minimize}~~L_{π_{→θ_0}}(π_→θ)~~\\textrm{subject to}~~D_\\textrm{KL}^{→θ_0}(π_{→θ_0} || π_→θ) < δ$$\n",
    "\n",
    "The parameters are updated using $→d_F = F(→θ)^{-1} ∇ L_{π_{→θ_0}}(π_→θ)$, utilizing the\n",
    "conjugate gradient algorithm as described earlier for TNPG (note that the\n",
    "algorithm was designed originally for TRPO and only later employed for TNPG).\n",
    "\n",
    "\n",
    "To guarantee improvement and respect the $D_\\textrm{KL}$ constraint, a line\n",
    "search is in fact performed. We start by the learning rate of\n",
    "$\\sqrt{δ/(→d_F^T F(→θ)^{-1} →d_F)}$ and shrink it exponentially until\n",
    "the constraint is satistifed and the objective improves.\n",
    "\n",
    "---\n",
    "# Trust Region Policy Optimization\n",
    "\n",
    "![w=30%,h=center](rllib_tasks.pdf)\n",
    "\n",
    "![w=100%](rllib_results.pdf)\n",
    "\n",
    "---\n",
    "section: PPO\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "A simplification of TRPO which can be implemented using a few lines of code.\n",
    "\n",
    "Let $r_t(→θ) ≝ \\frac{π(A_t|S_t; →θ)}{π(A_t|S_t; →θ_\\textrm{old})}$. PPO\n",
    "minimizes the objective\n",
    "$$L^\\textrm{CLIP}(→θ) ≝ 𝔼_t\\Big[\\min\\big(r_t(→θ) Â_t, \\operatorname{clip}(r_t(→θ), 1-ε, 1+ε) Â_t)\\big)\\Big].$$\n",
    "\n",
    "Such $L^\\textrm{CLIP}(→θ)$ is a lower (pessimistic) bound.\n",
    "\n",
    "![w=60%,h=center](ppo_clipping.pdf)\n",
    "\n",
    "---\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "The advantages $Â_t$ are additionally estimated using _generalized\n",
    "advantage estimation_. Instead of the usual\n",
    "$Â_t ≝ ∑_{i=0}^{T-t-1} γ^i R_{t+1+i} + γ^{T-t} V(S_T) - V(S_t)$\n",
    "the authors employ\n",
    "$$Â_t ≝ ∑_{i=0}^{T-t-1} (γλ)^i δ_{t+i},$$\n",
    "where $δ_t = R_{t+1} + γV(S_{t+1}) - V(S_t)$.\n",
    "\n",
    "![w=80%,h=center](ppo_algorithm.pdf)\n",
    "\n",
    "---\n",
    "# Proximal Policy Optimization\n",
    "\n",
    "![w=100%,v=middle](ppo_results.pdf)\n",
    "\n",
    "---\n",
    "section: SAC\n",
    "# Soft Actor Critic\n",
    "\n",
    "The paper Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement\n",
    "Learning with a Stochastic Actor by Tuomas Haarnoja et al. introduces\n",
    "a different off-policy algorithm for continuous action space.\n",
    "\n",
    "\n",
    "The general idea is to introduce entropy directly in the value function we want\n",
    "to maximize.\n",
    "\n",
    "---\n",
    "# Soft Actor Critic\n",
    "![w=60%,h=center](sac_algorithm.pdf)\n",
    "\n",
    "---\n",
    "# Soft Actor Critic\n",
    "![w=90%](sac_results.pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
