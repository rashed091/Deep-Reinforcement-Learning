{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process, Optimal Solutions, Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Process\n",
    "\n",
    "![w=55%,h=center](diagram.pdf)\n",
    "\n",
    "A _Markov decision process_ (MDP) is a quadruple $(𝓢, 𝓐, p, γ)$,\n",
    "where:\n",
    "- $𝓢$ is a set of states,\n",
    "\n",
    "- $𝓐$ is a set of actions,\n",
    "\n",
    "- $p(S_{t+1} = s', R_{t+1} = r | S_t = s, A_t = a)$ is a probability that\n",
    "  action $a ∈ 𝓐$ will lead from state $s ∈ 𝓢$ to $s' ∈ 𝓢$, producing a _reward_ $r ∈ ℝ$,\n",
    "\n",
    "- $γ ∈ [0, 1]$ is a _discount factor_.\n",
    "\n",
    "\n",
    "Let a _return_ $G_t$ be $G_t ≝ ∑_{k=0}^∞ γ^k R_{t + 1 + k}$. The goal is to optimize $𝔼[G_0]$.\n",
    "\n",
    "\n",
    "# Multi-armed Bandits as MDP\n",
    "\n",
    "To formulate $n$-armed bandits problem as MDP, we do not need states.\n",
    "Therefore, we could formulate it as:\n",
    "- one-element set of states, $𝓢=\\{S\\}$;\n",
    "\n",
    "- an action for every arm, $𝓐=\\{a_1, a_2, …, a_n\\}$;\n",
    "\n",
    "- assuming every arm produces rewards with a distribution of $𝓝(μ_i, σ_i^2)$,\n",
    "  the MDP dynamics function $p$ is defined as\n",
    "  $$p(S, r | S, a_i) = 𝓝(r | μ_i, σ_i^2).$$\n",
    "\n",
    "\n",
    "One possibility to introduce states in multi-armed bandits problem is to have\n",
    "separate reward distribution for every state. Such generalization is\n",
    "usually called _Contextualized Bandits_ problem.\n",
    "Assuming that state transitions are independent on rewards and given by\n",
    "a distribution $\\textit{next}(s)$, the MDP dynamics function for contextualized\n",
    "bandits problem is given by\n",
    "$$p(s', r | s, a_i) = 𝓝(r | μ_{i,s}, σ_{i,s}^2) ⋅ \\textit{next}(s'|s).$$\n",
    "\n",
    "---\n",
    "# (State-)Value and Action-Value Functions\n",
    "\n",
    "A _policy_ $π$ computes a distribution of actions in a given state, i.e.,\n",
    "$π(a | s)$ corresponds to a probability of performing an action $a$ in state\n",
    "$s$.\n",
    "\n",
    "\n",
    "To evaluate a quality of a policy, we define _value function_ $v_π(s)$, or\n",
    "_state-value function_, as\n",
    "$$v_π(s) ≝ 𝔼_π\\left[G_t \\middle| S_t = s\\right] = 𝔼_π\\left[∑\\nolimits_{k=0}^∞ γ^k R_{t+k+1} \\middle| S_t=s\\right].$$\n",
    "\n",
    "\n",
    "An _action-value function_ for a policy $π$ is defined analogously as\n",
    "$$q_π(s, a) ≝ 𝔼_π\\left[G_t \\middle| S_t = s, A_t = a\\right] = 𝔼_π\\left[∑\\nolimits_{k=0}^∞ γ^k R_{t+k+1} \\middle| S_t=s, A_t = a\\right].$$\n",
    "\n",
    "\n",
    "Evidently,\n",
    "$$\\begin{aligned}\n",
    "  v_π(s) &= 𝔼_π[q_π(s, a)], \\\\\n",
    "  q_π(s, a) &= 𝔼_π[R_{t+1} + γv_π(S_{t+1}) | S_t = s, A_t = a].\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Value Functions\n",
    "\n",
    "Optimal state-value function is defined as\n",
    "$$v_*(s) ≝ \\max_π v_π(s),$$\n",
    "analogously\n",
    "$$q_*(s, a) ≝ \\max_π q_π(s, a).$$\n",
    "\n",
    "\n",
    "Any policy $π_*$ with $v_{π_*} = v_*$ is called an _optimal policy_. Such policy\n",
    "can be defined as $π_*(s) ≝ \\argmax_a q_*(s, a) = \\argmax_a 𝔼[R_{t+1} + γv_*(S_{t+1}) | S_t = s, A_t = a]$.\n",
    "\n",
    "\n",
    "## Existence\n",
    "Under some mild assumptions, there always exists a unique optimal state-value function,\n",
    "unique optimal action-value function, and (not necessarily unique) optimal policy.\n",
    "The mild assumptions are that either termination is guaranteed from all\n",
    "reachable states, or $γ < 1$.\n",
    "\n",
    "\n",
    "section: Dynamic Programming\n",
    "# Dynamic Programming\n",
    "\n",
    "Dynamic programming is an approach devised by Richard Bellman in 1950s.\n",
    "\n",
    "\n",
    "To apply it to MDP, we now consider finite-horizon problems (i.e., with episodes\n",
    "of bounded length) with finite number of states $𝓢$ and actions $𝓐$, and known\n",
    "MDP dynamics $p$.\n",
    "\n",
    "\n",
    "The following recursion (which must hold for an optimal value function in a MDP,\n",
    "because future decisions does not depend on the current one) is usually called\n",
    "the _Bellman equation_:\n",
    "$$\\begin{aligned}\n",
    "  v_*(s) &= \\max_a 𝔼\\left[R_{t+1} + γ v_*(S_{t+1}) \\middle| S_t=s, A_t=a\\right] \\\\\n",
    "         &= \\max_a ∑_{s', r} p(s', r | s, a) \\left[r + γ v_*(s')\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "It can be also shown that if a value function satisfies the Bellman equation, it is\n",
    "already optimal.\n",
    "\n",
    "---\n",
    "# Dynamic Programming\n",
    "\n",
    "To turn the Bellman equation into an algorithm, we change the equal signs to assignments:\n",
    "$$\\begin{aligned}\n",
    "v_0(s) &← 0 \\\\\n",
    "v_{k+1}(s) &← \\max_a 𝔼\\left[R_{t+1} + γ v_k(S_{t+1}) \\middle| S_t=s, A_t=a\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "\n",
    "It is easy to show that if the problem consists of episodes of length at most\n",
    "$T$ steps, the optimal value function is reached after $T$ iteration of the\n",
    "above assignment (we can show by induction that $v_k(s)$ is the maximum return\n",
    "reachable from state $s$ in $k$ steps).\n",
    "\n",
    "---\n",
    "# Relations to Graph Algorithms\n",
    "\n",
    "Searching for optimal value functions of deterministic problems is in fact\n",
    "search for the shortest path in a suitable graph.\n",
    "\n",
    "\n",
    "![w=80%,mh=80%,h=center,v=middle](trellis.svg)\n",
    "\n",
    "---\n",
    "# Bellman-Ford-Moore Algorithm\n",
    "\n",
    "$$v_{k+1}(s) ← \\max_a 𝔼\\left[R_{t+1} + γ v_k(S_{t+1}) \\middle| S_t=s, A_t=a\\right].$$\n",
    "\n",
    "Bellman-Ford-Moore algorithm:\n",
    "```python\n",
    "# input: graph `g`, initial vertex `s`\n",
    "for v in g.vertices: d[v] = 0 if v == s else +∞\n",
    "\n",
    "for i in range(len(g.vertices) - 1):\n",
    "  for e in g.edges:\n",
    "    if d[e.source] + e.length < d[e.target]:\n",
    "      d[e.target] = d[e.source] + e.length\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "section: Value Iteration\n",
    "# Bellman Backup Operator\n",
    "\n",
    "Our goal is now to handle also infinite horizon tasks, using discount factor of\n",
    "$γ < 1$.\n",
    "\n",
    "\n",
    "For any value function $v∈ℝ^{|𝓢|}$ we define _Bellman backup operator_ $B : ℝ^{|𝓢|} → ℝ^{|𝓢|}$ as\n",
    "$$Bv(s) ≝ \\max_a 𝔼\\left[R_{t+1} + γ v(S_{t+1}) \\middle| S_t=s, A_t=a\\right].$$\n",
    "\n",
    "\n",
    "It is not difficult to show that Bellman backup operator is a _contraction_:\n",
    "$$\\max_s \\left|Bv_1(s) - Bv_2(s)\\right| ≤ γ \\max_s \\left|v_1(s) - v_2(s)\\right|.$$\n",
    "\n",
    "\n",
    "Considering a normed vector space $ℝ^{|𝓢|}$ with sup-norm $||⋅||_∞$,\n",
    "from Banach fixed-point theorem it follows there exist a _unique value function_\n",
    "$v_*$ such that\n",
    "$$Bv_* = v_*.$$\n",
    "\n",
    "\n",
    "Such unique $v_*$ is the _optimal value function_, because it satistifes the\n",
    "Bellman equation.\n",
    "\n",
    "---\n",
    "# Bellman Backup Operator\n",
    "\n",
    "Furthermore, iterative application of $B$ on arbitrary $v$ converges to $v_*$,\n",
    "because\n",
    "$$||Bv - v_*||_∞ = ||Bv - Bv_*||_∞ ≤ γ||v - v_*||,$$\n",
    "and therefore $B^nv → v_*$.\n",
    "\n",
    "---\n",
    "# Value Iteration Algorithm\n",
    "\n",
    "We can turn the iterative application of Bellman backup operator into an\n",
    "algorithm.\n",
    "$$Bv(s) ≝ \\max_a 𝔼\\left[R_{t+1} + γ v(S_{t+1}) \\middle| S_t=s, A_t=a\\right]$$\n",
    "\n",
    "![w=75%,h=center](value_iteration.pdf)\n",
    "\n",
    "---\n",
    "# Value Iteration Algorithm\n",
    "\n",
    "Although we have described the so-called _synchronous_ implementation requiring\n",
    "two arrays for $v$ and $Bv$, usual implementations are _asynchronous_ and modify\n",
    "the value function in place (if a fixed ordering is used, usually such value\n",
    "iteration is called _Gauss-Seidel_).\n",
    "\n",
    "\n",
    "Even with such asynchronous update value iteration can be proven to converge,\n",
    "and usually performs better in practise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Backup Operator as a Contraction\n",
    "\n",
    "To show that Bellman backup operator is a contraction, we proceed as follows:\n",
    "$$\\begin{aligned}\n",
    "||Bv_1 - Bv_2||_∞ &= ||\\max_a 𝔼\\left[R_{t+1} + γ v_1(S_{t+1})\\right] - \\max_a 𝔼\\left[R_{t+1} + γ v_2(S_{t+1})\\right]||_∞ \\\\\n",
    "                  &≤ \\max_a\\left( || 𝔼\\left[R_{t+1} + γ v_1(S_{t+1})\\right] - 𝔼\\left[R_{t+1} + γ v_2(S_{t+1})\\right]||_∞\\right) \\\\\n",
    "                  &= \\max_a\\left( \\left|\\left| ∑\\nolimits_{s', r} p\\left(s', r \\middle| s, a\\right)γ(v_1(s') - v_2(s'))\\right|\\right|_∞\\right) \\\\\n",
    "                  &= γ \\max_a\\left(\\left|\\left| ∑\\nolimits_{s', r} p\\left(s' \\middle| s, a\\right)(v_1(s') - v_2(s'))\\right|\\right|_∞\\right) \\\\\n",
    "                  &≤ γ ||v_1 - v_2||_∞,\n",
    "\\end{aligned}$$\n",
    "\n",
    "where the second line follows from $|\\max_x f(x) - \\max_x g(x)| ≤ \\max_x |f(x) - g(x)|$\n",
    "and the last line from the fact that from any given $s$ and $a$, the\n",
    "$∑_{s'} p(s' | s, a)$ sums to 1.\n",
    "\n",
    "---\n",
    "# Speed of Convergence\n",
    "\n",
    "Assuming maximum reward is $R_\\textrm{max}$, we have that\n",
    "$$v_*(s) ≤ ∑_{t=0}^∞ γ^t R_\\textrm{max} = \\frac{R_\\textrm{max}}{1-γ}.$$\n",
    "\n",
    "\n",
    "Starting with $v(s) ← 0$, we have\n",
    "$$||B^k v - v_*||_∞ ≤ γ^k ||v - v_*||_∞ = γ^k \\frac{R_\\textrm{max}}{1-γ}.$$\n",
    "\n",
    "\n",
    "Compare to finite horizon case, where $B^T v = v_*$.\n",
    "\n",
    "---\n",
    "section: Policy Iteration\n",
    "# Policy Iteration Algorithm\n",
    "\n",
    "We now propose another approach of computing optimal policy. The approach,\n",
    "called _policy iteration_, consists of repeatedly performing policy\n",
    "_evaluation_ and policy _improvement_.\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "Given a policy $π$, policy evaluation computes $v_π$.\n",
    "\n",
    "Recall that\n",
    "$$\\begin{aligned}\n",
    "  v_π(s) &≝ 𝔼_π\\left[G_t \\middle| S_t = s\\right] \\\\\n",
    "         &= 𝔼_π\\left[R_{t+1} + γ v_π(S_{t+1}) \\middle | S_t = s\\right] \\\\\n",
    "         &= ∑\\nolimits_a π(a|s) ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v_π(s')\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "If the dynamics of the MDP $p$ is known, the above is a system of linear\n",
    "equations, and therefore, $v_π$ can be computed exactly.\n",
    "\n",
    "---\n",
    "# Policy Evaluation\n",
    "The equation\n",
    "$$v_π(s) = ∑\\nolimits_a π(a|s) ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v_π(s')\\right]$$\n",
    "is called _Bellman equation for $v_π$_ and analogously to Bellman optimality\n",
    "equation, it can be proven that\n",
    "- under the same assumptions as before ($γ<1$ or termination), $v_π$ exists and\n",
    "  is unique;\n",
    "- $v_π$ is a fixed point of the Bellman equation\n",
    "  $$v_{k+1}(s) = ∑\\nolimits_a π(a|s) ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v_k(s')\\right];$$\n",
    "- iterative application of the Bellman equation to any $v$ converges to $v_π$.\n",
    "\n",
    "---\n",
    "class: middle\n",
    "# Policy Evaluation\n",
    "\n",
    "![w=100%](policy_evaluation.pdf)\n",
    "\n",
    "---\n",
    "# Policy Improvement\n",
    "\n",
    "Given $π$ and computed $v_π$, we would like to _improve_ the policy.\n",
    "A straightforward way to do so is to define a policy using a _greedy_ action\n",
    "$$\\begin{aligned}\n",
    "  π'(s) &≝ \\argmax_a q_π(s, a) \\\\\n",
    "        &= \\argmax_a ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v_π(s')\\right].\n",
    "\\end{aligned}$$\n",
    "\n",
    "For such $π'$, we can easily show that\n",
    "$$q_π(s, π'(s)) ≥ v_π(s).$$\n",
    "\n",
    "---\n",
    "# Policy Improvement Theorem\n",
    "\n",
    "Let $π$ and $π'$ be any pair of deterministic policies, such that\n",
    "$q_π(s, π'(s)) ≥ v_π(s)$.\n",
    "\n",
    "Then for all states $s$, $v_{π'}(s) ≥ v_π(s)$.\n",
    "\n",
    "\n",
    "The proof is straightforward, we repeatedly expand $q_π$ and use the\n",
    "assumption of the policy improvement theorem:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "v_π(s) &≤ q_π(s, π'(s)) \\\\\n",
    "       &= 𝔼[R_{t+1} + γ v_π(S_{t+1}) | S_t = s, A_t = π'(s)] \\\\\n",
    "       &= 𝔼_{π'}[R_{t+1} + γ v_π(S_{t+1}) | S_t = s] \\\\\n",
    "       &≤ 𝔼_{π'}[R_{t+1} + γ q_π(S_{t+1}, π'(S_{t+1})) | S_t = s] \\\\\n",
    "       &= 𝔼_{π'}[R_{t+1} + γ E[R_{t+2} + γ v_π(S_{t+2}) | S_{t+1}, A_{t+1} = π'(S_{t+1})] | S_t = s] \\\\\n",
    "       &= 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 v_π(S_{t+2}) | S_t = s] \\\\\n",
    "       &… \\\\\n",
    "       &≤ 𝔼_{π'}[R_{t+1} + γ R_{t+2} + γ^2 R_{t+3} + … | S_t = s] = v_{π'}(s)\n",
    "\\end{aligned}$$\n",
    "\n",
    "---\n",
    "# Policy Improvement Example\n",
    "\n",
    "![w=50%](gridworld_4x4.pdf)![w=60%,mw=50%,h=center](gridworld_4x4_policy_evaluation.pdf)\n",
    "\n",
    "---\n",
    "# Policy Iteration Algorithm\n",
    "\n",
    "Policy iteration consists of repeatedly performing policy evaluation and policy\n",
    "improvement:\n",
    "$$π_0 \\stackrel{E}{\\longrightarrow} v_{π_0} \\stackrel{I}{\\longrightarrow}\n",
    "  π_1 \\stackrel{E}{\\longrightarrow} v_{π_1} \\stackrel{I}{\\longrightarrow}\n",
    "  π_2 \\stackrel{E}{\\longrightarrow} v_{π_2} \\stackrel{I}{\\longrightarrow}\n",
    "  … \\stackrel{I}{\\longrightarrow} π_* \\stackrel{E}{\\longrightarrow} v_{π_*}.$$\n",
    "\n",
    "\n",
    "The result is a sequence of monotonically improving policies $π_i$. Note that\n",
    "when $π' = π$, also $v_{π'} = v_π$, which means Bellman optimality equation is\n",
    "fulfilled and both $v_π$ and $π$ are optimal.\n",
    "\n",
    "\n",
    "Considering that there is only a finite number of policies, the optimal policy\n",
    "and optimal value function can be computed in finite time (contrary to value\n",
    "iteration, where the convergence is only asymptotic).\n",
    "\n",
    "\n",
    "Note that when evaluating policy $π_{k+1}$, we usually start with $v_{π_k}$,\n",
    "which is assumed to be a good approximation to $v_{π_{k+1}}$.\n",
    "\n",
    "---\n",
    "# Policy Iteration Algorithm\n",
    "![w=70%,h=center](policy_iteration.pdf)\n",
    "\n",
    "---\n",
    "# Value Iteration as Policy Iteration\n",
    "\n",
    "Note that value iteration is in fact a policy iteration, where policy evaluation\n",
    "is performed only for one step:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "  π'(s) &= \\argmax_a ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v(s')\\right] &\\textit{(policy improvement)} \\\\\n",
    "  v'(s) &= ∑\\nolimits_a π'(a|s) ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v(s')\\right] &\\textit{(one step of policy evaluation)}\n",
    "\\end{aligned}$$\n",
    "\n",
    "Substituting the former into the latter, we get\n",
    "$$v'(s) = \\max_a ∑\\nolimits_{s', r} p(s', r | s, a) \\left[r + γ v(s)\\right] = Bv(s).$$\n",
    "\n",
    "---\n",
    "# Generalized Policy Iteration\n",
    "\n",
    "Therefore, it seems that to achieve convergence, it is not necessary to perform\n",
    "policy evaluation exactly.\n",
    "\n",
    "_Generalized Policy Evaluation_ is a general idea of interleaving policy\n",
    "evaluation and policy improvement at various granularity.\n",
    "\n",
    "\n",
    "![w=30%,mw=50%,h=center](gpi.pdf)![w=80%,mw=50%,h=center](gpi_convergence.pdf)\n",
    "\n",
    "If both processes stabilize, we know we have obtained optimal policy.\n",
    "\n",
    "\n",
    "section: Monte Carlo Methods\n",
    "# Monte Carlo Methods\n",
    "\n",
    "We now present the first algorithm for computing optimal policies without assuming\n",
    "a knowledge of the environment dynamics.\n",
    "\n",
    "However, we still assume there are finitely many states $𝓢$ and we will store\n",
    "estimates for each of them.\n",
    "\n",
    "\n",
    "Monte Carlo methods are based on estimating returns from complete episodes.\n",
    "Furthermore, if the model (of the environment) is not known, we need to\n",
    "estimate returns for the action-value function $q$ instead of $v$.\n",
    "\n",
    "\n",
    "We can formulate Monte Carlo methods in the generalized policy improvement\n",
    "framework.\n",
    "\n",
    "\n",
    "Keeping estimated returns for the action-value function, we perform policy\n",
    "evaluation by sampling one episode according to current policy. We then update\n",
    "the action-value function by averaging over the observed returns, including\n",
    "the sampled episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Methods\n",
    "\n",
    "To guarantee convergence, we need to visit each state infinitely many times.\n",
    "One of the simplest way to achieve that is to assume _exploring starts_, where\n",
    "we randomly select the first state and first action, each pair with nonzero\n",
    "probability.\n",
    "\n",
    "\n",
    "Furthermore, if a state-action pair appears multiple times in one episode, the\n",
    "sampled returns are not independent. The literature distinguishes two cases:\n",
    "- _first visit_: only the first occurence of a state-action pair in an episode is\n",
    "  considered\n",
    "- _every visit_: all occurences of a state-action pair are considered.\n",
    "\n",
    "Even though first-visit is easier to analyze, it can be proven that for both\n",
    "approaches, policy evaluation converges. Contrary to the Reinforcement Learning:\n",
    "An Introduction book, which presents first-visit algorithms, we use every-visit.\n",
    "\n",
    "---\n",
    "# Monte Carlo with Exploring Starts\n",
    "\n",
    "![w=90%,h=center](monte_carlo_exploring_starts.pdf)\n",
    "\n",
    "\n",
    "---\n",
    "# Monte Carlo and $ε$-soft Policies\n",
    "\n",
    "A policy is called $ε$-soft, if\n",
    "$$π(a|s) ≥ \\frac{ε}{|𝓐(s)|}.$$\n",
    "\n",
    "\n",
    "For $ε$-soft policy, Monte Carlo policy evaluation also converges, without the need\n",
    "of exploring starts.\n",
    "\n",
    "\n",
    "We call a policy $ε$-greedy, if one action has maximum probability of\n",
    "$1-ε+\\frac{ε}{|A(s)|}$.\n",
    "\n",
    "\n",
    "The policy improvement theorem can be proved also for the class of $ε$-soft\n",
    "policies, and using<br>$ε$-greedy policy in policy improvement step, policy\n",
    "iteration has the same convergence properties. (We can embed the $ε$-soft behaviour\n",
    "“inside” the environment and prove equivalence.)\n",
    "\n",
    "---\n",
    "# Monte Carlo for $ε$-soft Policies\n",
    "\n",
    "### On-policy every-visit Monte Carlo for $ε$-soft Policies\n",
    "Algorithm parameter: small $ε>0$\n",
    "\n",
    "Initialize $Q(s, a) ∈ ℝ$ arbitrarily (usually to 0), for all $s ∈ 𝓢, a ∈ 𝓐$<br>\n",
    "Initialize $C(s, a) ∈ ℤ$ to 0, for all $s ∈ 𝓢, a ∈ 𝓐$\n",
    "\n",
    "Repeat forever (for each episode):\n",
    "- Generate an episode $S_0, A_0, R_1, …, S_{T-1}, A_{T-1}, R_T$,\n",
    "  by generating actions as follows:\n",
    "  - With probability $ε$, generate a random uniform action\n",
    "  - Otherwise, set $A_t ≝ \\argmax\\nolimits_a Q(S_t, a)$\n",
    "- $G ← 0$\n",
    "- For each $t=T-1, T-2, …, 0$:\n",
    "  - $G ← γG + R_{T+1}$\n",
    "  - $C(S_t, A_t) ← C(S_t, A_t) + 1$\n",
    "  - $Q(S_t, A_t) ← Q(S_t, A_t) + \\frac{1}{C(S_t, A_t)}(G - Q(S_t, A_t))$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
